{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348639bf",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5bd6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install Important Libs\n",
    "%pip install -q datasets transformers pydantic matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1a173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Literal\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709352ef",
   "metadata": {},
   "source": [
    "# Environment Check before running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from typing import Optional, Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# --- Safe optional imports ---\n",
    "if importlib.util.find_spec(\"torch\"):\n",
    "    import torch\n",
    "else:\n",
    "    torch = None\n",
    "\n",
    "if importlib.util.find_spec(\"datasets\"):\n",
    "    import datasets\n",
    "else:\n",
    "    datasets = None\n",
    "\n",
    "if importlib.util.find_spec(\"transformers\"):\n",
    "    import transformers\n",
    "else:\n",
    "    transformers = None\n",
    "\n",
    "\n",
    "class LibraryVersions(BaseModel):\n",
    "    torch: str = \" Not Installed\"\n",
    "    datasets: str = \" Not Installed\"\n",
    "    transformers: str = \" Not Installed\"\n",
    "\n",
    "\n",
    "class GPUInfo(BaseModel):\n",
    "    device: Literal[\"GPU\", \"CPU\", \"No Device Detected\"]\n",
    "    cuda_version: Optional[str] = None\n",
    "    gpu_name: Optional[str] = None\n",
    "    memory_gb: Optional[float] = None\n",
    "    message: Optional[str] = None\n",
    "\n",
    "\n",
    "class EnvironmentCheck:\n",
    "    \"\"\"Environment checker with Pydantic models.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.torch_version = torch.__version__ if torch else None\n",
    "        self.datasets_version = datasets.__version__ if datasets else None\n",
    "        self.transformers_version = transformers.__version__ if transformers else None\n",
    "\n",
    "    def get_versions(self) -> LibraryVersions:\n",
    "        \"\"\"Return installed library versions as a Pydantic model.\"\"\"\n",
    "        return LibraryVersions(\n",
    "            torch=self.torch_version or \" Not Installed\",\n",
    "            datasets=self.datasets_version or \" Not Installed\",\n",
    "            transformers=self.transformers_version or \" Not Installed\",\n",
    "        )\n",
    "\n",
    "    def check_gpu(self) -> GPUInfo:\n",
    "        \"\"\"Return GPU details as a Pydantic model.\"\"\"\n",
    "        if not torch:\n",
    "            return GPUInfo(device=\"No Device Detected\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            props = torch.cuda.get_device_properties(0)\n",
    "            return GPUInfo(\n",
    "                device=\"GPU\",\n",
    "                cuda_version=torch.version.cuda,\n",
    "                gpu_name=torch.cuda.get_device_name(0),\n",
    "                memory_gb=round(props.total_memory / (1024**3), 2),\n",
    "            )\n",
    "        return GPUInfo(device=\"CPU\", message=\"No GPU detected\")\n",
    "\n",
    "\n",
    "# --- Usage for manual run ---\n",
    "if __name__ == \"__main__\":\n",
    "    env = EnvironmentCheck()\n",
    "    print(\"Library Versions:\", env.get_versions().model_dump())\n",
    "    print(\"Device Info:\", env.check_gpu().model_dump())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8814623",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "\n",
    "class DatasetConfig(BaseModel):\n",
    "    \"\"\"Configuration model for dataset loading.\"\"\"\n",
    "    dataset_name: str = Field(..., description=\"Name of the dataset to load (e.g., 'wikitext').\")\n",
    "    subset_name: str = Field(..., description=\"Subset/config name (e.g., 'wikitext-2-raw-v1').\")\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "    \"\"\"Wrapper class for loading HuggingFace datasets.\"\"\"\n",
    "\n",
    "    def __init__(self, config: DatasetConfig) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    def load_data(self) -> DatasetDict:\n",
    "        \"\"\"Load dataset using HuggingFace.\"\"\"\n",
    "        dataset = load_dataset(self.config.dataset_name, self.config.subset_name)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "config = DatasetConfig(dataset_name=\"wikitext\", subset_name=\"wikitext-2-raw-v1\")\n",
    "loader = DatasetLoader(config)\n",
    "dataset = loader.load_data()\n",
    "\n",
    "print(dataset)\n",
    "for i in range(10):\n",
    "    print(f\"Row {i}: {dataset['train'][i]['text']!r}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59dea62",
   "metadata": {},
   "source": [
    "# Environment Setup and Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430e52f",
   "metadata": {},
   "source": [
    "ðŸ”¹ Environment Setup\n",
    "\n",
    "Before training a language model, we must ensure the environment is ready:\n",
    "\n",
    "Libraries: PyTorch for neural networks, HuggingFace datasets for loading corpora, and transformers for tokenization.\n",
    "\n",
    "GPU Availability: Training transformers on CPU is not practical. We verified CUDA version, GPU type, and VRAM size.\n",
    "\n",
    "Reproducibility: Using a structured check (EnvironmentCheck), we can confirm dependencies and hardware setup are consistent across machines.\n",
    "\n",
    "ðŸ”¹ Dataset Preparation in LLM Training\n",
    "\n",
    "Large-scale models are trained on diverse, massive corpora:\n",
    "\n",
    "ðŸŒ Common Crawl â†’ large-scale web scrape (cleaned and deduplicated).\n",
    "\n",
    "ðŸ“š Books â†’ public-domain and licensed.\n",
    "\n",
    "ðŸ“ Wikipedia â†’ factual, well-structured text.\n",
    "\n",
    "ðŸ’» Code â†’ GitHub, forums, Q&A.\n",
    "\n",
    "ðŸ§ª Research Articles â†’ scientific sources like arXiv and PubMed.\n",
    "\n",
    "Such datasets often reach billions to trillions of tokens. Preprocessing includes deduplication, filtering, and quality checks.\n",
    "\n",
    "ðŸ”¹ Our Choice for EduMoE\n",
    "\n",
    "For this educational project, we use:\n",
    "\n",
    "âœ… WikiText-2 (raw):\n",
    "\n",
    "~2M tokens â€” small enough for fast experiments.\n",
    "\n",
    "Natural, factual prose similar to real-world corpora.\n",
    "\n",
    "Already split into train, validation, and test.\n",
    "\n",
    "Includes quirks like blank lines, which helps mimic real preprocessing challenges.\n",
    "\n",
    "This dataset is ideal for learning the mechanics of LLM training without overwhelming compute resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21aa56",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class TokenizerConfig(BaseModel):\n",
    "    \"\"\"Configuration for tokenizer setup.\"\"\"\n",
    "    tokenizer_name: str = Field(default=\"gpt2\", description=\"Pretrained tokenizer to use.\")\n",
    "    add_special_tokens: bool = Field(default=False, description=\"Whether to add special tokens like BOS/EOS.\")\n",
    "\n",
    "\n",
    "class TokenizerWrapper:\n",
    "    \"\"\"Wrapper for HuggingFace GPT-2 tokenizer.\"\"\"\n",
    "\n",
    "    def __init__(self, config: TokenizerConfig) -> None:\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "        # GPT-2 has no pad token by default â†’ we assign EOS as pad for batching\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert text â†’ token IDs.\"\"\"\n",
    "        return self.tokenizer.encode(text, add_special_tokens=self.config.add_special_tokens)\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        \"\"\"Convert token IDs â†’ text.\"\"\"\n",
    "        return self.tokenizer.decode(tokens)\n",
    "\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"Return vocabulary size.\"\"\"\n",
    "        return self.tokenizer.vocab_size\n",
    "        \n",
    "\n",
    "# --- Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    config = TokenizerConfig(tokenizer_name=\"gpt2\", add_special_tokens=False)\n",
    "    tok = TokenizerWrapper(config)\n",
    "\n",
    "    text = \"EduMoE is our Mixture of Experts project.\"\n",
    "    tokens = tok.encode(text)\n",
    "    decoded = tok.decode(tokens)\n",
    "\n",
    "    print(\"Vocab size:\", tok.vocab_size())\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed66d6d1",
   "metadata": {},
   "source": [
    "# Tokenize WikiText-2 with Our Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c1d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load WikiText-2 (raw)\n",
    "data_config = DatasetConfig(dataset_name=\"wikitext\", subset_name=\"wikitext-2-raw-v1\")\n",
    "dataset_loader = DatasetLoader(data_config)\n",
    "dataset = dataset_loader.load_data()\n",
    "\n",
    "# 2. Load GPT-2 Byte-Level BPE tokenizer\n",
    "tok_config = TokenizerConfig(tokenizer_name=\"gpt2\", add_special_tokens=False)\n",
    "tokenizer = TokenizerWrapper(tok_config)\n",
    "\n",
    "# 3. Grab a non-empty sample from training set\n",
    "sample_text = next(x[\"text\"] for x in dataset[\"train\"] if x[\"text\"].strip() != \"\")\n",
    "\n",
    "# 4. Encode â†’ token IDs\n",
    "token_ids = tokenizer.encode(sample_text)\n",
    "\n",
    "# 5. Decode â†’ back to text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "\n",
    "# 6. Print results\n",
    "print(\"Original text:\\n\", sample_text[:200], \"\\n\")\n",
    "print(\"Token IDs (first 40):\\n\", token_ids[:40], \"...\\n\")\n",
    "print(\"Decoded text:\\n\", decoded_text[:200], \"\\n\")\n",
    "print(\"Char length vs Token length:\", len(sample_text), \"chars â†’\", len(token_ids), \"tokens\")\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c959d3b8",
   "metadata": {},
   "source": [
    "# Before implementing MOE , We will be implementing a decoder style transformer and then convert it into a MOE Type design\n",
    "# This is how a gpt-2 style model looks like\n",
    "![Capture](images/Capture.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8b848",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "![emblookup](images/emblookup.PNG)\n",
    "![learn_para](images/learn_para.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee9f88a",
   "metadata": {},
   "source": [
    "# We will using nn.embedding as a safe wrapper for lookup table and otherwise\n",
    "![nn.emb.PNG](images/nn.emb.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b373af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmbeddingConfig(BaseModel):\n",
    "    \"\"\"Configuration for token embeddings.\"\"\"\n",
    "    vocab_size: int = Field(..., description=\"Number of tokens in the vocabulary\")\n",
    "    embedding_dim: int = Field(..., description=\"Dimension of each embedding vector\")\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    \"\"\"Embedding layer for tokens.\"\"\"\n",
    "\n",
    "    def __init__(self, config: EmbeddingConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.embedding_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Map token IDs to embedding vectors.\n",
    "        Args:\n",
    "            token_ids (Tensor): shape (batch_size, seq_len)\n",
    "        Returns:\n",
    "            Tensor: shape (batch_size, seq_len, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.embedding(token_ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ae8297",
   "metadata": {},
   "source": [
    "# Connecting everything till now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557f89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load WikiText-2 dataset\n",
    "data_config = DatasetConfig(dataset_name=\"wikitext\", subset_name=\"wikitext-2-raw-v1\")\n",
    "dataset_loader = DatasetLoader(data_config)\n",
    "dataset = dataset_loader.load_data()\n",
    "\n",
    "# 2. Load tokenizer (GPT-2 BBPE)\n",
    "tok_config = TokenizerConfig(tokenizer_name=\"gpt2\", add_special_tokens=False)\n",
    "tokenizer = TokenizerWrapper(tok_config)\n",
    "\n",
    "# 3. Get vocab size from tokenizer\n",
    "vocab_size = tokenizer.vocab_size()\n",
    "embedding_dim = 128   # keep small for demo, GPT-2 uses 768\n",
    "\n",
    "# 4. Create embedding layer\n",
    "embed_config = EmbeddingConfig(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "embed_layer = TokenEmbedding(embed_config)\n",
    "\n",
    "# 5. Take one sample from dataset (non-empty)\n",
    "sample_text = next(x[\"text\"] for x in dataset[\"train\"] if x[\"text\"].strip() != \"\")\n",
    "print(\"Original text:\", sample_text[:100], \"\\n\")\n",
    "\n",
    "# 6. Tokenize â†’ IDs\n",
    "token_ids = tokenizer.encode(sample_text)\n",
    "token_tensor = torch.tensor([token_ids])  # add batch dim\n",
    "\n",
    "print(\"Token IDs:\", token_ids[:20], \"...\\n\")\n",
    "\n",
    "# 7. Pass through embeddings\n",
    "vectors = embed_layer(token_tensor)\n",
    "print(\"Embeddings shape:\", vectors.shape)\n",
    "print(\"First token vector (truncated):\", vectors[0, 0, :10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a13ead0",
   "metadata": {},
   "source": [
    "# positional embeddings\n",
    "\n",
    "# Formula\n",
    "![pos_emb](images/pos_emb.PNG)\n",
    "\n",
    "# Benifits\n",
    "![pos_emb_1](images/pos_emb_1.PNG)\n",
    "\n",
    "# the sinusoidal positional encoding (original Transformer, Vaswani et al. 2017) is not learnable.\n",
    "\n",
    "# Lately system use somehting known Rotary Position Embeddings (RoPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class PositionalEncodingConfig(BaseModel):\n",
    "    \"\"\"Configuration for sinusoidal positional encoding.\"\"\"\n",
    "    embedding_dim: int = Field(..., description=\"Embedding dimension (same as token embeddings)\")\n",
    "    max_len: int = Field(5000, description=\"Maximum sequence length (context window)\")\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Sinusoidal positional encoding (Vaswani et al. 2017).\"\"\"\n",
    "\n",
    "    def __init__(self, config: PositionalEncodingConfig) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        d_model = config.embedding_dim\n",
    "        max_len = config.max_len\n",
    "\n",
    "        # Create matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Position indices: (max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Different frequencies for each dimension (0,2,4,...)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Apply sin to even indices, cos to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add batch dimension: (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register as buffer â†’ not learnable, moves with model to GPU\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor (batch_size, seq_len, embedding_dim)\n",
    "        Returns:\n",
    "            Tensor with positional encoding added\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        return x + self.pe[:, :seq_len, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6bd477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token embeddings first\n",
    "embed_config = EmbeddingConfig(vocab_size=50257, embedding_dim=128)\n",
    "embed_layer = TokenEmbedding(embed_config)\n",
    "\n",
    "# Positional encodings\n",
    "pos_config = PositionalEncodingConfig(embedding_dim=128, max_len=512)\n",
    "pos_encoder = PositionalEncoding(pos_config)\n",
    "\n",
    "# Example input: 1 sentence, 9 tokens\n",
    "token_ids = torch.tensor([[796, 569, 18354, 7496, 17740, 6711, 796, 220, 198]])\n",
    "token_vectors = embed_layer(token_ids)\n",
    "\n",
    "print(\"Before Positional Encoding:\", token_vectors[0, 0, :5])\n",
    "\n",
    "# Add positional encodings\n",
    "out = pos_encoder(token_vectors)\n",
    "\n",
    "print(\"After Positional Encoding:\", out[0, 0, :5])\n",
    "print(\"Output shape:\", out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6604074a",
   "metadata": {},
   "source": [
    "# Before moving towards multihead attention and other forms of attention best is to reivse a simple feed forward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3265c614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate data\n",
    "torch.manual_seed(42)  # reproducibility\n",
    "x = torch.linspace(-2*torch.pi, 2*torch.pi, 200).unsqueeze(1)  # shape (200, 1)\n",
    "y = torch.sin(x)  # target\n",
    "\n",
    "# visualize\n",
    "plt.scatter(x.numpy(), y.numpy(), label=\"True function\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40093ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input layer â†’ hidden layer (ReLU) â†’ output layer\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleFFN(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=16, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),  # (1 -> 16) #Here there 16 neurons in the hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)  # (16 -> 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4236cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = SimpleFFN(input_dim=1, hidden_dim=16, output_dim=1)\n",
    "\n",
    "# pick a sample x\n",
    "x_sample = torch.tensor([[1.0]])  # shape (1,1)\n",
    "y_pred = model(x_sample)\n",
    "\n",
    "print(\"Input:\", x_sample.item())\n",
    "print(\"Model output (untrained):\", y_pred.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Loss function and Optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb7d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "epochs = 1000\n",
    "for epoch in range(epochs):\n",
    "    # forward pass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    \n",
    "    # backward pass\n",
    "    optimizer.zero_grad()   # reset gradients\n",
    "    loss.backward()         # compute gradients\n",
    "    optimizer.step()        # update weights\n",
    "    \n",
    "    # log every 100 epochs\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379913ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model predictions after training\n",
    "with torch.no_grad():  # no gradients needed\n",
    "    y_pred = model(x)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x.numpy(), y.numpy(), label=\"True sin(x)\", color=\"blue\", alpha=0.5)\n",
    "plt.plot(x.numpy(), y_pred.numpy(), label=\"FFN predictions\", color=\"red\", linewidth=2)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58981cd",
   "metadata": {},
   "source": [
    "# Full Code for an FFN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a95278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Feedforward Network (FFN) Example\n",
    "# Task: Learn sin(x)\n",
    "# ---------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Dataset\n",
    "torch.manual_seed(42)\n",
    "x = torch.linspace(-2*torch.pi, 2*torch.pi, 200).unsqueeze(1)   # shape (200, 1)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 2. Define FFN\n",
    "class SimpleFFN(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=32, output_dim=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "model = SimpleFFN()\n",
    "\n",
    "# 3. Loss + Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# 4. Training loop\n",
    "epochs = 10000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # forward\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# 5. Plot loss curve\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training Loss Curve\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Plot predictions vs true\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x.numpy(), y.numpy(), label=\"True sin(x)\", color=\"blue\", alpha=0.5)\n",
    "plt.plot(x.numpy(), y_pred.numpy(), label=\"FFN predictions\", color=\"red\", linewidth=2)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441ecf64",
   "metadata": {},
   "source": [
    "# From Basic FFN to Modern FFN\n",
    "\n",
    "The above was a **basic FFN**. Lately, things have changed a bit â€” the **heart is still the same**, but modern architectures add some refinements:\n",
    "\n",
    "---\n",
    "\n",
    "###  Step A â€“ RMSNorm (Pre-Norm)\n",
    "- Normalize the input **before** passing into the FFN.  \n",
    "- Use **RMSNorm** (lighter, used in *LLaMA* / *Mistral*) instead of LayerNorm.  \n",
    "\n",
    "---\n",
    "\n",
    "###  Step B â€“ SwiGLU (Modern Gating)\n",
    "- Replace the plain MLP with **SwiGLU** for richer dynamics.  \n",
    "- Adjust hidden dimension properly so parameter count â‰ˆ classic 4Ã—.  \n",
    "\n",
    "---\n",
    "\n",
    "### Step C â€“ Dropout (Two Places)\n",
    "- **Inside FFN** (after activation/gating).  \n",
    "- **After final projection** (residual dropout).  \n",
    "\n",
    "---\n",
    "\n",
    "###  Step D â€“ DropPath (Optional, Training Stabilizer)\n",
    "- Randomly skip whole FFN blocks during training.  \n",
    "- Helps with regularization in very deep networks.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515c52c",
   "metadata": {},
   "source": [
    "# ðŸ”¹ RMSNorm (Pre-Norm) â€” Theory\n",
    "\n",
    "### LayerNorm (baseline)\n",
    "$$\n",
    "\\text{LN}(x) = \\frac{x - \\mu}{\\sigma} \\cdot \\gamma + \\beta\n",
    "$$\n",
    "\n",
    "- Subtracts the mean $\\mu$ and divides by standard deviation $\\sigma$ across features.  \n",
    "- Has learnable parameters: $\\gamma$ (scale) and $\\beta$ (shift).  \n",
    "- Works well, but slightly heavier due to mean calculation.  \n",
    "\n",
    "---\n",
    "\n",
    "### RMSNorm (modern choice)\n",
    "$$\n",
    "\\text{RMSNorm}(x) = \\frac{x}{\\text{RMS}(x)} \\cdot \\gamma\n",
    "$$\n",
    "\n",
    "where  \n",
    "\n",
    "$$\n",
    "\\text{RMS}(x) = \\sqrt{\\frac{1}{d} \\sum_{i=1}^{d} x_i^2 + \\epsilon}\n",
    "$$\n",
    "\n",
    "- **No mean subtraction** (lighter than LayerNorm).  \n",
    "- Only rescales by Root Mean Square (RMS).  \n",
    "- Uses **scale parameter $\\gamma$** (no shift $\\beta$).  \n",
    "- Faster, simpler, and often just as effective.  \n",
    "\n",
    "---\n",
    "\n",
    "### Pre-Norm vs Post-Norm\n",
    "- **Post-Norm (old style, original Transformer):**  \n",
    "## x â†’ FFN â†’ Norm â†’ (add residual)  (Earlier)\n",
    "### x â†’ Norm â†’ FFN â†’ (add residual)  (Now)\n",
    "\n",
    "- **Pre-Norm helps stabilize very deep networks and avoids vanishing gradients.**\n",
    "\n",
    "\n",
    "âœ… Modern FFNs use **RMSNorm in Pre-Norm style**, making them lighter and more stable than the ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ee43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        # learnable scale parameter Î³ (same size as hidden dimension)\n",
    "        self.scale = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # compute RMS over last dimension\n",
    "        rms = x.pow(2).mean(dim=-1, keepdim=True).sqrt()  # shape: (batch, seq, 1)\n",
    "        x_normed = x / (rms + self.eps)\n",
    "        return self.scale * x_normed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0e8544",
   "metadata": {},
   "source": [
    "# ðŸ”¹ Modern FFN (Step A: Add RMSNorm Pre-Norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b501d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# custom RMSNorm (from previous step)\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(dim))  # learnable Î³\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = x.pow(2).mean(dim=-1, keepdim=True).sqrt()\n",
    "        x_normed = x / (rms + self.eps)\n",
    "        return self.scale * x_normed\n",
    "\n",
    "# FFN with RMSNorm Pre-Norm\n",
    "class ModernFFN(nn.Module):\n",
    "    def __init__(self, d_model=16, d_ff=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm = RMSNorm(d_model)  # Pre-Norm\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=False),\n",
    "            nn.ReLU(),   # will replace with SwiGLU in Step B\n",
    "            nn.Linear(d_ff, d_model, bias=False),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-Norm\n",
    "        x_normed = self.norm(x)\n",
    "        out = self.ffn(x_normed)\n",
    "        return self.dropout(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1e5879",
   "metadata": {},
   "source": [
    "# Train with RMS Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c5a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# 1. Dataset\n",
    "# ---------------------------\n",
    "torch.manual_seed(42)\n",
    "x = torch.linspace(-2*torch.pi, 2*torch.pi, 200).unsqueeze(1)   # (200,1)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. RMSNorm definition\n",
    "# ---------------------------\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.scale = nn.Parameter(torch.ones(dim))  # Î³\n",
    "    def forward(self, x):\n",
    "        rms = x.pow(2).mean(dim=-1, keepdim=True).sqrt()\n",
    "        return self.scale * x / (rms + self.eps)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. FFN with RMSNorm Pre-Norm\n",
    "# ---------------------------\n",
    "class ModernFFN(nn.Module):\n",
    "    def __init__(self, d_model=1, d_ff=64, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.norm = RMSNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff, bias=True),  # keep bias for toy task\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model, bias=True),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_normed = self.norm(x)\n",
    "        return self.dropout(self.ffn(x_normed))\n",
    "\n",
    "model = ModernFFN()\n",
    "\n",
    "# ---------------------------\n",
    "# 4. Loss + Optimizer\n",
    "# ---------------------------\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# ---------------------------\n",
    "# 5. Training loop\n",
    "# ---------------------------\n",
    "epochs = 10000\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_pred = model(x)\n",
    "    loss = criterion(y_pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ---------------------------\n",
    "# 6. Plot predictions\n",
    "# ---------------------------\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(x.numpy(), y.numpy(), label=\"True sin(x)\", color=\"blue\", alpha=0.5)\n",
    "plt.plot(x.numpy(), y_pred.numpy(), label=\"FFN (RMSNorm)\", color=\"red\", linewidth=2)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24014186",
   "metadata": {},
   "source": [
    "# âš ï¸ Why RMSNorm Fails in Our Tiny 1D Toy Example\n",
    "\n",
    "- In our setup, the input dimension is only **1**.  \n",
    "- RMSNorm computes:\n",
    "  $$\n",
    "  \\text{RMS}(x) = \\sqrt{\\frac{1}{d}\\sum_{i=1}^d x_i^2 + \\epsilon}\n",
    "  $$\n",
    "  When $d=1$, this reduces to just:\n",
    "  $$\n",
    "  \\text{RMS}(x) \\approx |x|\n",
    "  $$\n",
    "- So RMSNorm normalizes every input to roughly **Â±1**, destroying the scale information the model needs to learn $\\sin(x)$.  \n",
    "\n",
    "---\n",
    "\n",
    "### Additional Factors\n",
    "- We also disabled **biases** in Linear layers (bias=False) for modern FFNs, but in a **tiny regression task**, biases are actually needed to shift outputs.  \n",
    "- **Dropout** (even small) can harm training when the network and dataset are this small.  \n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Takeaway\n",
    "- RMSNorm is designed for **high-dimensional embeddings** (e.g., 512, 1024, 4096 in Transformers).  \n",
    "- In such cases, the RMS over many features preserves enough information while stabilizing training.  \n",
    "- In our **1D toy case**, RMSNorm collapses variation â†’ the model outputs a flat line.  \n",
    "\n",
    "ðŸ‘‰ For demonstration of RMSNorm, we should use **larger embedding sizes** (like in Transformers), not 1D toy regressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bb57c4",
   "metadata": {},
   "source": [
    "# Bahdanau Attention (2014) â€” Neural Machine Translation\n",
    "\n",
    "**Problem (pre-attention):**  \n",
    "In vanilla encoderâ€“decoder RNNs, the entire input sequence was compressed into a **single fixed-length vector** (the final hidden state of the encoder).  \n",
    "- This worked for short sentences but failed badly for long ones.  \n",
    "- Information from early words was lost.  \n",
    "- Translation quality degraded as input length increased.  \n",
    "\n",
    "---\n",
    "\n",
    "### Core Idea (Bahdanau et al., 2014)\n",
    "Instead of relying only on the last encoder hidden state, let the decoder:\n",
    "1. **Keep all encoder hidden states** \\( h_1, h_2, â€¦, h_T \\).  \n",
    "2. At each decoder step \\( t \\), **decide which encoder states to focus on** by computing an *alignment score*.  \n",
    "3. Use these scores to build a weighted average of encoder states = **context vector** \\( c_t \\).  \n",
    "4. Combine \\( c_t \\) with the decoder state \\( s_t \\) to make the next prediction.  \n",
    "\n",
    "This is called **soft alignment** because it distributes attention weights across all input words.\n",
    "\n",
    "---\n",
    "\n",
    "### Step-by-Step Math\n",
    "\n",
    "1. **Alignment scores**  \n",
    "For each decoder step \\( t \\), compare decoder hidden state \\( s_t \\) with every encoder hidden state \\( h_i \\):  \n",
    "\n",
    "$$\n",
    "e_{t,i} = v_a^\\top \\tanh(W_s s_t + W_h h_i)\n",
    "$$  \n",
    "\n",
    "- \\( W_s, W_h, v_a \\) are learnable parameters.  \n",
    "- \\( e_{t,i} \\) = relevance of encoder word \\( i \\) when predicting the next output.  \n",
    "\n",
    "---\n",
    "\n",
    "2. **Attention weights (softmax normalization)**  \n",
    "\n",
    "$$\n",
    "\\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_j \\exp(e_{t,j})}\n",
    "$$  \n",
    "\n",
    "Now \\(\\alpha_{t,i}\\) indicates *how much attention to give* to word \\( i \\).  \n",
    "\n",
    "---\n",
    "\n",
    "3. **Context vector**  \n",
    "\n",
    "$$\n",
    "c_t = \\sum_i \\alpha_{t,i} \\cdot h_i\n",
    "$$  \n",
    "\n",
    "This is a dynamic summary of the source, focused on what matters for step \\( t \\).  \n",
    "\n",
    "---\n",
    "\n",
    "4. **Attended state**  \n",
    "\n",
    "$$\n",
    "\\tilde{s}_t = \\tanh(W_c [s_t ; c_t])\n",
    "$$  \n",
    "\n",
    "This merges â€œwhat the decoder knows so farâ€ with â€œwhat it just focused on.â€  \n",
    "\n",
    "---\n",
    "\n",
    "5. **Output prediction**  \n",
    "\n",
    "$$\n",
    "y_t = \\text{softmax}(W_o \\tilde{s}_t)\n",
    "$$  \n",
    "\n",
    "---\n",
    "\n",
    "### Intuition Recap\n",
    "- Encoder: produces a hidden state for each input word.  \n",
    "- Decoder step \\( t \\):  \n",
    "  - Computes alignment with each encoder state.  \n",
    "  - Builds a context vector as a weighted blend.  \n",
    "  - Uses this to predict the next word.  \n",
    "\n",
    "**Key difference from pre-attention:** Instead of a single static summary, the decoder dynamically decides *where to look* at every step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502b52f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, attn_size):\n",
    "        super().__init__()\n",
    "        # Linear layers for computing alignment scores\n",
    "        self.W_s = nn.Linear(hidden_size, attn_size)   # for decoder state\n",
    "        self.W_h = nn.Linear(hidden_size, attn_size)   # for encoder states\n",
    "        self.v_a = nn.Linear(attn_size, 1, bias=False) # score to scalar\n",
    "\n",
    "    def forward(self, decoder_state, encoder_outputs):\n",
    "        \"\"\"\n",
    "        decoder_state: (batch, hidden_size) at time t\n",
    "        encoder_outputs: (batch, seq_len, hidden_size) for all encoder steps\n",
    "        \"\"\"\n",
    "        # (batch, 1, hidden_size)\n",
    "        dec = decoder_state.unsqueeze(1)\n",
    "\n",
    "        # Project both\n",
    "        dec_proj = self.W_s(dec)                        # (batch, 1, attn_size)\n",
    "        enc_proj = self.W_h(encoder_outputs)            # (batch, seq_len, attn_size)\n",
    "\n",
    "        # Broadcast addition + tanh\n",
    "        scores = self.v_a(torch.tanh(dec_proj + enc_proj))  # (batch, seq_len, 1)\n",
    "\n",
    "        # Drop the last dim\n",
    "        scores = scores.squeeze(-1)  # (batch, seq_len)\n",
    "\n",
    "        # Softmax over seq_len â†’ attention weights\n",
    "        attn_weights = F.softmax(scores, dim=1)  # (batch, seq_len)\n",
    "\n",
    "        # Weighted sum of encoder outputs â†’ context vector\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs) \n",
    "        context = context.squeeze(1)  # (batch, hidden_size)\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "# ----------------------------\n",
    "# Demo\n",
    "# ----------------------------\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "hidden_size = 8\n",
    "attn_size = 6\n",
    "\n",
    "# Fake encoder outputs (like for \"I love India\")\n",
    "encoder_outputs = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "# Fake decoder hidden state (at some step t)\n",
    "decoder_state = torch.randn(batch_size, hidden_size)\n",
    "\n",
    "# Run attention\n",
    "attention = BahdanauAttention(hidden_size, attn_size)\n",
    "context, attn_weights = attention(decoder_state, encoder_outputs)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights)\n",
    "print(\"Context vector:\", context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c12c7e",
   "metadata": {},
   "source": [
    "# Luong (2015) Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22680b2",
   "metadata": {},
   "source": [
    "# Bahdanau (2014) vs. Luong (2015) Attention\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŒ± Bahdanau Attention (Additive Attention, 2014)\n",
    "\n",
    "**Key idea:** Use a small feed-forward neural network to compute *alignment scores*.  \n",
    "- He called this **additive attention**.\n",
    "\n",
    "**Steps:**\n",
    "1. **Alignment score:**\n",
    "   $$\n",
    "   e_{t,i} = v_a^\\top \\tanh(W_s s_t + W_h h_i)\n",
    "   $$\n",
    "   - Learnable parameters: $W_s, W_h, v_a$  \n",
    "   - Nonlinear scoring function.\n",
    "\n",
    "2. **Attention weights:**\n",
    "   $$\n",
    "   \\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_j \\exp(e_{t,j})}\n",
    "   $$\n",
    "\n",
    "3. **Context vector:**\n",
    "   $$\n",
    "   c_t = \\sum_i \\alpha_{t,i} h_i\n",
    "   $$\n",
    "\n",
    "4. **Combine context + decoder state:**\n",
    "   $$\n",
    "   \\tilde{s}_t = \\tanh(W_c [s_t ; c_t])\n",
    "   $$\n",
    "\n",
    "5. **Predict next word:**\n",
    "   $$\n",
    "   y_t = \\text{softmax}(W_o \\tilde{s}_t)\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŒ¿ Luong Attention (Multiplicative Attention, 2015)\n",
    "\n",
    "**Key idea:** Simplify scoring with similarity measures.  \n",
    "- He called this **multiplicative attention**.  \n",
    "- Two main scoring variants:  \n",
    "  - **Dot:** $e_{t,i} = s_t^\\top h_i$  \n",
    "  - **General:** $e_{t,i} = s_t^\\top W h_i$\n",
    "\n",
    "**Steps:**\n",
    "1. **Alignment score (dot/general):**\n",
    "   - Dot product or linear transform, no extra hidden layer.\n",
    "\n",
    "2. **Attention weights:** *(same as Bahdanau)*  \n",
    "   $$\n",
    "   \\alpha_{t,i} = \\frac{\\exp(e_{t,i})}{\\sum_j \\exp(e_{t,j})}\n",
    "   $$\n",
    "\n",
    "3. **Context vector:** *(same as Bahdanau)*  \n",
    "   $$\n",
    "   c_t = \\sum_i \\alpha_{t,i} h_i\n",
    "   $$\n",
    "\n",
    "4. **Combine context + decoder state:**  \n",
    "   $$\n",
    "   \\tilde{h}_t = \\tanh(W_c [c_t ; s_t])\n",
    "   $$\n",
    "\n",
    "5. **Predict next word:** *(same as Bahdanau)*  \n",
    "   $$\n",
    "   y_t = \\text{softmax}(W_o \\tilde{h}_t)\n",
    "   $$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŒ Main Differences\n",
    "\n",
    "- **Scoring function:**  \n",
    "  - Bahdanau â†’ additive MLP with nonlinearity.  \n",
    "  - Luong â†’ dot product (fast) or linear â€œgeneralâ€ form.  \n",
    "\n",
    "- **Efficiency:**  \n",
    "  - Bahdanau slower, more parameters.  \n",
    "  - Luong lighter, faster, easier to scale.  \n",
    "\n",
    "- **Local vs. global:**  \n",
    "  - Luong also introduced **local attention**, focusing only on a window of encoder states, improving speed on long sequences.  \n",
    "\n",
    "**Shared parts:**  \n",
    "Both compute attention weights â†’ context vector â†’ combine with decoder state â†’ predict next word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1160c0",
   "metadata": {},
   "source": [
    "# Global vs Local Attention (Luong, 2015)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŒ Global Attention\n",
    "- At each decoder step \\(t\\), compute alignment scores between decoder state \\(s_t\\) and **all encoder states** \\(h_1, h_2, â€¦, h_S\\).  \n",
    "- Apply softmax over the full sequence to get attention weights.  \n",
    "- Build context vector as weighted sum of **all encoder states**.  \n",
    "- Advantage: model can attend anywhere.  \n",
    "- Disadvantage: expensive for long sequences (\\(O(S)\\) per step) and focus may spread too thin.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” Local Attention\n",
    "- Predict a **center position** \\(p_t\\) in the source sequence.  \n",
    "  - **Monotonic (local-m):**  \n",
    "    $$\n",
    "    p_t = t\n",
    "    $$\n",
    "  - **Predictive (local-p):**  \n",
    "    $$\n",
    "    p_t = S \\cdot \\sigma(v_p^\\top \\tanh(W_p s_t))\n",
    "    $$\n",
    "    where a small neural net predicts \\(p_t\\) from the decoder state.\n",
    "\n",
    "- Apply a **Gaussian prior** around \\(p_t\\):  \n",
    "  $$\n",
    "  G_{t,i} = \\exp\\!\\Big(-\\frac{(i - p_t)^2}{2\\sigma^2}\\Big)\n",
    "  $$\n",
    "\n",
    "- Final unnormalized attention:  \n",
    "  $$\n",
    "  \\alpha_{t,i} \\propto \\exp(e_{t,i}) \\cdot G_{t,i}\n",
    "  $$\n",
    "\n",
    "- Effect: model attends strongly near \\(p_t\\), but softly includes neighbors.  \n",
    "- Advantage: cheaper, sharper focus, better for long sequences.  \n",
    "- Disadvantage: risk of missing distant dependencies if \\(p_t\\) is predicted poorly.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Intuition\n",
    "- **Global:** a wide searchlight scanning the entire source sentence.  \n",
    "- **Local:** a focused spotlight predicted by the decoder, softly centered by a Gaussian curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408347c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        # Luong \"general\" score uses a linear transform of encoder states\n",
    "        self.W = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "\n",
    "    def score(self, decoder_state, encoder_outputs):\n",
    "        \"\"\"\n",
    "        decoder_state: (batch, hidden_size)\n",
    "        encoder_outputs: (batch, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        # (batch, hidden_size) -> (batch, 1, hidden_size)\n",
    "        dec = decoder_state.unsqueeze(1)\n",
    "        # Apply linear to encoder outputs (general form)\n",
    "        enc = self.W(encoder_outputs)  # (batch, seq_len, hidden_size)\n",
    "        # Dot product along hidden dim\n",
    "        scores = torch.bmm(dec, enc.transpose(1, 2))  # (batch, 1, seq_len)\n",
    "        return scores.squeeze(1)  # (batch, seq_len)\n",
    "\n",
    "    def forward(self, decoder_state, encoder_outputs):\n",
    "        # Step 1: compute alignment scores\n",
    "        scores = self.score(decoder_state, encoder_outputs)  # (batch, seq_len)\n",
    "\n",
    "        # Step 2: softmax -> attention weights\n",
    "        attn_weights = F.softmax(scores, dim=1)  # (batch, seq_len)\n",
    "\n",
    "        # Step 3: weighted sum -> context vector\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        context = context.squeeze(1)  # (batch, hidden_size)\n",
    "\n",
    "        return context, attn_weights\n",
    "\n",
    "# ----------------------------\n",
    "# Demo\n",
    "# ----------------------------\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "hidden_size = 8\n",
    "\n",
    "# Fake encoder outputs (like for \"I love India\")\n",
    "encoder_outputs = torch.randn(batch_size, seq_len, hidden_size)\n",
    "\n",
    "# Fake decoder hidden state (at some step t)\n",
    "decoder_state = torch.randn(batch_size, hidden_size)\n",
    "\n",
    "# Run attention\n",
    "attention = LuongAttention(hidden_size)\n",
    "context, attn_weights = attention(decoder_state, encoder_outputs)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights)\n",
    "print(\"Context vector:\", context)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b370773",
   "metadata": {},
   "source": [
    "# Self-Attention (Scaled Dot-Product Attention)\n",
    "\n",
    "---\n",
    "\n",
    "### Motivation\n",
    "- RNNs process tokens **sequentially** â†’ slow, hard to parallelize.  \n",
    "- Self-Attention lets each token **attend to all others directly** â†’ full parallelism on GPUs.  \n",
    "\n",
    "---\n",
    "\n",
    "### Step 1 â€” Queries, Keys, Values\n",
    "Each token embedding \\(x_i \\in \\mathbb{R}^d\\) is projected into:\n",
    "\n",
    "$$\n",
    "q_i = W_Q x_i, \\quad k_i = W_K x_i, \\quad v_i = W_V x_i\n",
    "$$\n",
    "\n",
    "- **Query**: what this token is asking for.  \n",
    "- **Key**: what this token can offer.  \n",
    "- **Value**: the actual information to share.  \n",
    "\n",
    "---\n",
    "\n",
    "### Step 2 â€” Similarity Scores\n",
    "Compare Query of token \\(i\\) against Keys of all tokens \\(j\\):\n",
    "\n",
    "$$\n",
    "\\text{score}(i,j) = \\frac{q_i^\\top k_j}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "Scaling by \\(\\sqrt{d_k}\\) prevents large dot products from destabilizing softmax.\n",
    "\n",
    "---\n",
    "\n",
    "### Step 3 â€” Attention Weights\n",
    "Convert scores into probabilities:\n",
    "\n",
    "$$\n",
    "\\alpha_{i,j} = \\frac{\\exp(\\text{score}(i,j))}{\\sum_m \\exp(\\text{score}(i,m))}\n",
    "$$\n",
    "\n",
    "\\(\\alpha_{i,j}\\) = how much token \\(i\\) attends to token \\(j\\).  \n",
    "\n",
    "---\n",
    "\n",
    "### Step 4 â€” Context Vector\n",
    "Each token builds a new representation as a weighted sum of Values:\n",
    "\n",
    "$$\n",
    "z_i = \\sum_j \\alpha_{i,j} v_j\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Step 5 â€” Matrix Form\n",
    "For the whole sequence (length \\(n\\)):\n",
    "\n",
    "$$\n",
    "Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "- Output \\(Z \\in \\mathbb{R}^{n \\times d_v}\\).  \n",
    "- Each row \\(z_i\\) = contextual embedding for token \\(i\\).  \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b063122d",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_Q = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.W_K = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.W_V = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (batch, seq_len, embed_size)\n",
    "        \"\"\"\n",
    "        Q = self.W_Q(X)   # (batch, seq_len, embed_size)\n",
    "        K = self.W_K(X)   # (batch, seq_len, embed_size)\n",
    "        V = self.W_V(X)   # (batch, seq_len, embed_size)\n",
    "\n",
    "        # Step 1: Compute similarity scores\n",
    "        # QK^T -> (batch, seq_len, seq_len)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.embed_size ** 0.5)\n",
    "\n",
    "        # Step 2: Softmax to get attention weights\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # (batch, seq_len, seq_len)\n",
    "\n",
    "        # Step 3: Weighted sum with V\n",
    "        out = torch.matmul(attn_weights, V)  # (batch, seq_len, embed_size)\n",
    "\n",
    "        return out, attn_weights\n",
    "\n",
    "# ----------------------------\n",
    "# Demo\n",
    "# ----------------------------\n",
    "batch_size = 1\n",
    "seq_len = 3\n",
    "embed_size = 4\n",
    "\n",
    "# Fake token embeddings (\"I\", \"love\", \"India\")\n",
    "X = torch.randn(batch_size, seq_len, embed_size)\n",
    "\n",
    "self_attn = SelfAttention(embed_size)\n",
    "out, attn_weights = self_attn(X)\n",
    "\n",
    "print(\"Attention weights:\\n\", attn_weights)\n",
    "print(\"\\nOutput embeddings:\\n\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a2f15",
   "metadata": {},
   "source": [
    "# Multi-Head Attention (MHA)\n",
    "\n",
    "## ðŸŽ¯ Motivation\n",
    "\n",
    "- **Single-head attention** learns only one type of relation between tokens\n",
    "- Language has **multiple simultaneous relations**:\n",
    "  - Subjectâ€“verb dependencies\n",
    "  - Objectâ€“verb relationships  \n",
    "  - Positional patterns\n",
    "  - Semantic roles\n",
    "- **Multi-Head Attention** = multiple attention \"specialists\" working in parallel\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Mathematical Framework\n",
    "\n",
    "### Step 1: Per-Head Projections\n",
    "For each head $i = 1, \\ldots, h$, project input embeddings $X \\in \\mathbb{R}^{n \\times d}$:\n",
    "\n",
    "$$Q^i = XW_Q^i, \\quad K^i = XW_K^i, \\quad V^i = XW_V^i$$\n",
    "\n",
    "where $W_Q^i, W_K^i, W_V^i \\in \\mathbb{R}^{d \\times d_k}$ and typically $d_k = \\frac{d}{h}$.\n",
    "\n",
    "### Step 2: Scaled Dot-Product Attention\n",
    "Each head computes attention independently:\n",
    "\n",
    "$$Z^i = \\text{softmax}\\left(\\frac{Q^i (K^i)^\\top}{\\sqrt{d_k}}\\right)V^i$$\n",
    "\n",
    "Output: $Z^i \\in \\mathbb{R}^{n \\times d_v}$ where $d_v = d_k$ typically.\n",
    "\n",
    "### Step 3: Concatenation\n",
    "Concatenate all heads along the feature dimension:\n",
    "\n",
    "$$Z = [Z^1 \\,;\\, Z^2 \\,;\\, \\ldots \\,;\\, Z^h] \\in \\mathbb{R}^{n \\times (h \\cdot d_v)}$$\n",
    "\n",
    "### Step 4: Final Linear Projection\n",
    "Project back to embedding dimension:\n",
    "\n",
    "$$\\text{MHA}(X) = ZW_O$$\n",
    "\n",
    "where $W_O \\in \\mathbb{R}^{(h \\cdot d_v) \\times d}$.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ Architecture Flow\n",
    "\n",
    "```\n",
    "Input Embeddings X âˆˆ â„â¿Ë£áµˆ\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚   Linear Projections    â”‚\n",
    "â”‚  W_Q^i, W_K^i, W_V^i    â”‚\n",
    "â”‚    for i = 1...h        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Head 1  â”‚  Head 2  â”‚  Head h  â”‚\n",
    "â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚ â”‚QÂ¹KÂ¹VÂ¹â”‚ â”‚ â”‚QÂ²KÂ²VÂ²â”‚ â”‚ â”‚QÊ°KÊ°VÊ°â”‚ â”‚\n",
    "â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚ Attn(Â·)  â”‚ Attn(Â·)  â”‚ Attn(Â·)  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "    Concatenate\n",
    "   [ZÂ¹; ZÂ²; ...; ZÊ°]\n",
    "         â”‚\n",
    "         â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Final Linear Layer     â”‚\n",
    "â”‚        W_O              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "         â”‚\n",
    "         â–¼\n",
    "   MHA Output âˆˆ â„â¿Ë£áµˆ\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Key Insights\n",
    "\n",
    "1. **Parallel Processing**: Each head learns different attention patterns simultaneously\n",
    "2. **Complementary Views**: Heads may specialize in:\n",
    "   - Syntactic dependencies (subject-verb)\n",
    "   - Semantic relationships (word meanings)\n",
    "   - Positional patterns (nearby tokens)\n",
    "   - Long-range dependencies\n",
    "\n",
    "3. **Parameter Efficiency**: By using $d_k = d/h$, total parameters â‰ˆ single-head attention\n",
    "\n",
    "4. **Expressiveness**: $h$ heads can capture $h$ different types of relationships\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Implementation Notes\n",
    "\n",
    "- **Typical values**: $h = 8$ or $h = 12$ in practice\n",
    "- **Dimension splitting**: $d_k = d_v = d/h$ keeps parameter count manageable  \n",
    "- **Computational complexity**: $O(n^2 d)$ same as single-head, but with $h$Ã— parallelization\n",
    "- **Memory**: Stores $h$ attention matrices of size $n \\times n$\n",
    "\n",
    "---\n",
    "\n",
    "*Next: Implement MHA in PyTorch with shape annotations and visualization!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a33617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_size, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_size % num_heads == 0, \"embed_size must be divisible by num_heads\"\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_size // num_heads  # dimension per head\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_Q = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.W_K = nn.Linear(embed_size, embed_size, bias=False)\n",
    "        self.W_V = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "        # Final linear projection\n",
    "        self.W_O = nn.Linear(embed_size, embed_size, bias=False)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: (batch, seq_len, embed_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = X.size()\n",
    "\n",
    "        # Project into Q, K, V\n",
    "        Q = self.W_Q(X)  # (batch, seq_len, embed_size)\n",
    "        K = self.W_K(X)\n",
    "        V = self.W_V(X)\n",
    "\n",
    "        # Reshape into multiple heads: (batch, num_heads, seq_len, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Step 1: Compute scaled dot-product scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        # scores: (batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # Step 2: Softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1)  # (batch, num_heads, seq_len, seq_len)\n",
    "\n",
    "        # Step 3: Weighted sum with V\n",
    "        out = torch.matmul(attn_weights, V)  # (batch, num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Step 4: Concatenate heads back\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_size)\n",
    "\n",
    "        # Step 5: Final projection\n",
    "        out = self.W_O(out)  # (batch, seq_len, embed_size)\n",
    "\n",
    "        return out, attn_weights\n",
    "\n",
    "# ----------------------------\n",
    "# Demo\n",
    "# ----------------------------\n",
    "batch_size = 1\n",
    "seq_len = 4\n",
    "embed_size = 8\n",
    "num_heads = 2\n",
    "\n",
    "# Fake input (4 tokens, each embed_size=8)\n",
    "X = torch.randn(batch_size, seq_len, embed_size)\n",
    "\n",
    "mha = MultiHeadAttention(embed_size, num_heads)\n",
    "out, attn_weights = mha(X)\n",
    "\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # (batch, heads, seq_len, seq_len)\n",
    "print(\"Output shape:\", out.shape)                      # (batch, seq_len, embed_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d53ec",
   "metadata": {},
   "source": [
    "# Final Code for GPT-2\n",
    "\n",
    "# max_position_embeddings = 512 â†’ theoretical maximum context length.\n",
    "# The embedding table is built for 512 positions, so the model could handle sequences that long.\n",
    "\n",
    "# block_size = 256 â†’ practical training context length.\n",
    "# During training, you only ever feed chunks of 256 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9ff349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration parameters.\"\"\"\n",
    "    # Model config\n",
    "    vocab_size: int = 50257  #vocab for tokenizer\n",
    "    max_position_embeddings: int = 512\n",
    "    n_embd: int = 256\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 8\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # Training config\n",
    "    batch_size: int = 8\n",
    "    learning_rate: float = 3e-4\n",
    "    max_epochs: int = 10\n",
    "    warmup_steps: int = 1000\n",
    "    max_steps: Optional[int] = None\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Data config\n",
    "    block_size: int = 256  # sequence length for training\n",
    "    \n",
    "    # Logging\n",
    "    eval_interval: int = 500\n",
    "    log_interval: int = 100\n",
    "    save_interval: int = 1000\n",
    "\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    \"\"\"Dataset for WikiText-2 with proper tokenization and chunking.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], tokenizer, block_size: int = 256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Tokenize all texts and concatenate\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if text.strip():  # Skip empty lines\n",
    "                tokens = tokenizer.encode(text.strip())\n",
    "                all_tokens.extend(tokens)\n",
    "                all_tokens.append(tokenizer.eos_token_id)  # Add EOS between articles\n",
    "        \n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Dataset has {len(self.tokens)} tokens\")\n",
    "        \n",
    "        # Calculate number of samples\n",
    "        self.num_samples = max(0, len(self.tokens) - block_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a chunk of tokens\n",
    "        chunk = self.tokens[idx:idx + self.block_size + 1]\n",
    "        \n",
    "        # Input and target (shifted by 1)\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "\n",
    "class TrainingManager:\n",
    "    \"\"\"Manages the training process.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load and prepare data\n",
    "        self.train_loader, self.val_loader = self._prepare_data()\n",
    "        \n",
    "        # Initialize model\n",
    "        model_config = self._create_model_config()\n",
    "        self.model = GPT2Model(model_config).to(self.device)\n",
    "        print(f\"Model has {self.model.get_num_params():,} parameters\")\n",
    "        \n",
    "        # Initialize optimizer and scheduler\n",
    "        self.optimizer = self._create_optimizer()\n",
    "        self.scheduler = self._create_scheduler()\n",
    "        \n",
    "        # Training state\n",
    "        self.step = 0\n",
    "        self.epoch = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def _create_model_config(self):\n",
    "        \"\"\"Create model configuration from training config.\"\"\"\n",
    "        from types import SimpleNamespace\n",
    "        return SimpleNamespace(\n",
    "            vocab_size=self.config.vocab_size,\n",
    "            max_position_embeddings=self.config.max_position_embeddings,\n",
    "            n_embd=self.config.n_embd,\n",
    "            n_layer=self.config.n_layer,\n",
    "            n_head=self.config.n_head,\n",
    "            n_inner=None,\n",
    "            dropout=self.config.dropout,\n",
    "            layer_norm_epsilon=1e-5,\n",
    "            use_bias=True\n",
    "        )\n",
    "    \n",
    "    def _prepare_data(self) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"Load and prepare WikiText-2 dataset.\"\"\"\n",
    "        print(\"Loading WikiText-2 dataset...\")\n",
    "        dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_texts = [item['text'] for item in dataset['train'] if item['text'].strip()]\n",
    "        val_texts = [item['text'] for item in dataset['validation'] if item['text'].strip()]\n",
    "        \n",
    "        train_dataset = WikiTextDataset(train_texts, self.tokenizer, self.config.block_size)\n",
    "        val_dataset = WikiTextDataset(val_texts, self.tokenizer, self.config.block_size)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "        print(f\"Val dataset: {len(val_dataset)} samples\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def _create_optimizer(self) -> optim.Optimizer:\n",
    "        \"\"\"Create AdamW optimizer with weight decay.\"\"\"\n",
    "        # Separate parameters for weight decay\n",
    "        decay_params = []\n",
    "        no_decay_params = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if any(nd in name for nd in ['bias', 'ln', 'norm']):\n",
    "                    no_decay_params.append(param)\n",
    "                else:\n",
    "                    decay_params.append(param)\n",
    "        \n",
    "        optimizer_groups = [\n",
    "            {'params': decay_params, 'weight_decay': self.config.weight_decay},\n",
    "            {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        \n",
    "        return optim.AdamW(optimizer_groups, lr=self.config.learning_rate)\n",
    "    \n",
    "    def _create_scheduler(self):\n",
    "        \"\"\"Create learning rate scheduler with warmup.\"\"\"\n",
    "        def lr_lambda(step):\n",
    "            if step < self.config.warmup_steps:\n",
    "                return step / self.config.warmup_steps\n",
    "            return 1.0\n",
    "        \n",
    "        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
    "    \n",
    "    def train_step(self, batch) -> float:\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        self.model.train()\n",
    "        x, y = batch\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(x, labels=y)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / self.config.gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss.item() * self.config.gradient_accumulation_steps\n",
    "    \n",
    "    def evaluate(self) -> float:\n",
    "        \"\"\"Evaluate model on validation set.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                x, y = batch\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                \n",
    "                outputs = self.model(x, labels=y)\n",
    "                loss = outputs['loss']\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Limit evaluation for faster training\n",
    "                if num_batches >= 50:\n",
    "                    break\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def save_checkpoint(self, filepath: str):\n",
    "        \"\"\"Save training checkpoint.\"\"\"\n",
    "        checkpoint = {\n",
    "            'step': self.step,\n",
    "            'epoch': self.epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'config': self.config\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "    \n",
    "    def generate_sample(self, prompt: str = \"The\", max_tokens: int = 50) -> str:\n",
    "        \"\"\"Generate a sample text during training.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        input_ids = torch.tensor([self.tokenizer.encode(prompt)]).to(self.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            generated = self.model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.8,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Main training loop.\"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Model parameters: {self.model.get_num_params():,}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        accumulated_loss = 0\n",
    "        \n",
    "        for epoch in range(self.config.max_epochs):\n",
    "            self.epoch = epoch\n",
    "            \n",
    "            for batch_idx, batch in enumerate(self.train_loader):\n",
    "                # Training step\n",
    "                loss = self.train_step(batch)\n",
    "                accumulated_loss += loss\n",
    "                \n",
    "                # Update parameters every gradient_accumulation_steps\n",
    "                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(),\n",
    "                        self.config.max_grad_norm\n",
    "                    )\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    self.step += 1\n",
    "                    \n",
    "                    # Logging\n",
    "                    if self.step % self.config.log_interval == 0:\n",
    "                        avg_loss = accumulated_loss / self.config.log_interval\n",
    "                        self.train_losses.append(avg_loss)\n",
    "                        \n",
    "                        lr = self.scheduler.get_last_lr()[0]\n",
    "                        elapsed = time.time() - start_time\n",
    "                        \n",
    "                        print(f\"Step {self.step:5d} | \"\n",
    "                              f\"Epoch {epoch:2d} | \"\n",
    "                              f\"Loss: {avg_loss:.4f} | \"\n",
    "                              f\"LR: {lr:.2e} | \"\n",
    "                              f\"Time: {elapsed:.1f}s\")\n",
    "                        \n",
    "                        accumulated_loss = 0\n",
    "                    \n",
    "                    # Evaluation\n",
    "                    if self.step % self.config.eval_interval == 0:\n",
    "                        val_loss = self.evaluate()\n",
    "                        self.val_losses.append(val_loss)\n",
    "                        \n",
    "                        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "                        \n",
    "                        # Save best model\n",
    "                        if val_loss < self.best_val_loss:\n",
    "                            self.best_val_loss = val_loss\n",
    "                            self.save_checkpoint(f\"best_model_step_{self.step}.pt\")\n",
    "                            print(f\"New best model saved! Val loss: {val_loss:.4f}\")\n",
    "                        \n",
    "                        # Generate sample\n",
    "                        sample = self.generate_sample(\"The future of artificial intelligence\")\n",
    "                        print(f\"Sample: {sample[:100]}...\")\n",
    "                        print(\"-\" * 80)\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    if self.step % self.config.save_interval == 0:\n",
    "                        self.save_checkpoint(f\"checkpoint_step_{self.step}.pt\")\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    if self.config.max_steps and self.step >= self.config.max_steps:\n",
    "                        print(f\"Reached max steps ({self.config.max_steps})\")\n",
    "                        return\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        self.save_checkpoint(\"final_model.pt\")\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        \"\"\"Plot training and validation losses.\"\"\"\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Training loss\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(self.train_losses)\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Steps (x100)')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Validation loss\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(self.val_losses)\n",
    "        plt.title('Validation Loss')\n",
    "        plt.xlabel('Evaluations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Simplified GPT-2 model (use your full implementation)\n",
    "class GPT2Model(nn.Module):\n",
    "    \"\"\"Simplified GPT-2 for training demo.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embeddings\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.h = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        # Final norm and head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Tie weights\n",
    "        self.lm_head.weight = self.wte.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        b, t = input_ids.size()\n",
    "        pos = torch.arange(0, t, device=input_ids.device).unsqueeze(0)\n",
    "        \n",
    "        tok_emb = self.wte(input_ids)\n",
    "        pos_emb = self.wpe(pos)\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return {'logits': logits, 'loss': loss}\n",
    "    \n",
    "    def generate(self, input_ids, max_new_tokens=50, temperature=1.0, do_sample=True):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                outputs = self.forward(input_ids)\n",
    "                logits = outputs['logits'][:, -1, :] / temperature\n",
    "                \n",
    "                if do_sample:\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                else:\n",
    "                    next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                \n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        return input_ids\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(config.max_position_embeddings, config.max_position_embeddings)).view(1, 1, config.max_position_embeddings, config.max_position_embeddings))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "        att = att.masked_fill(self.causal_mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        return self.c_proj(y)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Main Training Script\n",
    "# ================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    # Create training configuration\n",
    "    config = TrainingConfig(\n",
    "        # Model config (small for quick training)\n",
    "        n_embd=256,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        \n",
    "        # Training config\n",
    "        batch_size=4,  # Small batch for demo\n",
    "        learning_rate=1e-4,\n",
    "        max_epochs=3,\n",
    "        warmup_steps=500,\n",
    "        max_steps=20000,  # Limit for demo\n",
    "        gradient_accumulation_steps=2,\n",
    "        \n",
    "        # Data config\n",
    "        block_size=128,  # Smaller sequences for faster training\n",
    "        \n",
    "        # Logging\n",
    "        eval_interval=200,\n",
    "        log_interval=50,\n",
    "        save_interval=500\n",
    "    )\n",
    "    \n",
    "    # Create trainer and start training\n",
    "    trainer = TrainingManager(config)\n",
    "    trainer.train()\n",
    "    \n",
    "    # Plot results\n",
    "    trainer.plot_losses()\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal Evaluation:\")\n",
    "    final_val_loss = trainer.evaluate()\n",
    "    print(f\"Final validation loss: {final_val_loss:.4f}\")\n",
    "    \n",
    "    # Generate some samples\n",
    "    print(\"\\nGenerated Samples:\")\n",
    "    for prompt in [\"The\", \"In the future\", \"Artificial intelligence\"]:\n",
    "        sample = trainer.generate_sample(prompt, max_tokens=30)\n",
    "        print(f\"Prompt: '{prompt}' -> {sample}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0b445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jupytext\n",
    "\n",
    "# Load the notebook\n",
    "# nb = jupytext.read(\"MOE.ipynb\")\n",
    "\n",
    "# Convert and write to .py\n",
    "jupytext.write(nb, \"MOE.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216466c",
   "metadata": {},
   "source": [
    "# GPT 2 From scratch improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff821fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "1.Increased block size to 512 to capture dependicies better\n",
    "2.Increased batch size , gradient accumaltion\n",
    "3.Updated learning rate scheduler to use cosine decay after warmup (as suggested by DeepMind manager)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration parameters.\"\"\"\n",
    "    # Model config\n",
    "    vocab_size: int = 50257  #vocab for tokenizer\n",
    "    max_position_embeddings: int = 512\n",
    "    n_embd: int = 256\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 8\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # Training config\n",
    "    batch_size: int = 8\n",
    "    learning_rate: float = 3e-4\n",
    "    max_epochs: int = 10\n",
    "    warmup_steps: int = 1000\n",
    "    max_steps: Optional[int] = None\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Data config\n",
    "    block_size: int = 512   \n",
    "    \n",
    "    # Logging\n",
    "    eval_interval: int = 500\n",
    "    log_interval: int = 100\n",
    "    save_interval: int = 1000\n",
    "\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    \"\"\"Dataset for WikiText-2 with proper tokenization and chunking.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], tokenizer, block_size: int = 256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Tokenize all texts and concatenate\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if text.strip():  # Skip empty lines\n",
    "                tokens = tokenizer.encode(text.strip())\n",
    "                all_tokens.extend(tokens)\n",
    "                all_tokens.append(tokenizer.eos_token_id)  # Add EOS between articles\n",
    "        \n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Dataset has {len(self.tokens)} tokens\")\n",
    "        \n",
    "        # Calculate number of samples\n",
    "        self.num_samples = max(0, len(self.tokens) - block_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a chunk of tokens\n",
    "        chunk = self.tokens[idx:idx + self.block_size + 1]\n",
    "        \n",
    "        # Input and target (shifted by 1)\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "\n",
    "class TrainingManager:\n",
    "    \"\"\"Manages the training process.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load and prepare data\n",
    "        self.train_loader, self.val_loader = self._prepare_data()\n",
    "        \n",
    "        # Initialize model\n",
    "        model_config = self._create_model_config()\n",
    "        self.model = GPT2Model(model_config).to(self.device)\n",
    "        print(f\"Model has {self.model.get_num_params():,} parameters\")\n",
    "        \n",
    "        # Initialize optimizer and scheduler\n",
    "        self.optimizer = self._create_optimizer()\n",
    "        self.scheduler = self._create_scheduler()\n",
    "        \n",
    "        # Training state\n",
    "        self.step = 0\n",
    "        self.epoch = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.learning_rates = []  # Track LR for plotting\n",
    "    \n",
    "    def _create_model_config(self):\n",
    "        \"\"\"Create model configuration from training config.\"\"\"\n",
    "        from types import SimpleNamespace\n",
    "        return SimpleNamespace(\n",
    "            vocab_size=self.config.vocab_size,\n",
    "            max_position_embeddings=self.config.max_position_embeddings,\n",
    "            n_embd=self.config.n_embd,\n",
    "            n_layer=self.config.n_layer,\n",
    "            n_head=self.config.n_head,\n",
    "            n_inner=None,\n",
    "            dropout=self.config.dropout,\n",
    "            layer_norm_epsilon=1e-5,\n",
    "            use_bias=True\n",
    "        )\n",
    "    \n",
    "    def _prepare_data(self) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"Load and prepare WikiText-2 dataset.\"\"\"\n",
    "        print(\"Loading WikiText-2 dataset...\")\n",
    "        dataset = load_dataset(\"wikitext\", \"\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_texts = [item['text'] for item in dataset['train'] if item['text'].strip()]\n",
    "        val_texts = [item['text'] for item in dataset['validation'] if item['text'].strip()]\n",
    "        \n",
    "        train_dataset = WikiTextDataset(train_texts, self.tokenizer, self.config.block_size)\n",
    "        val_dataset = WikiTextDataset(val_texts, self.tokenizer, self.config.block_size)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "        print(f\"Val dataset: {len(val_dataset)} samples\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def _create_optimizer(self) -> optim.Optimizer:\n",
    "        \"\"\"Create AdamW optimizer with weight decay.\"\"\"\n",
    "        # Separate parameters for weight decay\n",
    "        decay_params = []\n",
    "        no_decay_params = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if any(nd in name for nd in ['bias', 'ln', 'norm']):\n",
    "                    no_decay_params.append(param)\n",
    "                else:\n",
    "                    decay_params.append(param)\n",
    "        \n",
    "        optimizer_groups = [\n",
    "            {'params': decay_params, 'weight_decay': self.config.weight_decay},\n",
    "            {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        \n",
    "        return optim.AdamW(optimizer_groups, lr=self.config.learning_rate)\n",
    "    \n",
    "    def _create_scheduler(self):\n",
    "        \"\"\"Create learning rate scheduler with warmup + cosine decay.\"\"\"\n",
    "        # Calculate total steps for cosine decay\n",
    "        if self.config.max_steps:\n",
    "            total_steps = self.config.max_steps\n",
    "        else:\n",
    "            total_steps = len(self.train_loader) * self.config.max_epochs // self.config.gradient_accumulation_steps\n",
    "        \n",
    "        print(f\"Total training steps: {total_steps}\")\n",
    "        print(f\"Warmup steps: {self.config.warmup_steps}\")\n",
    "        print(f\"Cosine decay steps: {total_steps - self.config.warmup_steps}\")\n",
    "        \n",
    "        def lr_lambda(step):\n",
    "            if step < self.config.warmup_steps:\n",
    "                # Linear warmup\n",
    "                return step / self.config.warmup_steps\n",
    "            else:\n",
    "                # Cosine decay from 1.0 to 0.0\n",
    "                progress = (step - self.config.warmup_steps) / (total_steps - self.config.warmup_steps)\n",
    "                progress = min(progress, 1.0)  # Clamp to avoid going beyond 1.0\n",
    "                return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
    "    \n",
    "    def train_step(self, batch) -> float:\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        self.model.train()\n",
    "        x, y = batch\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(x, labels=y)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / self.config.gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss.item() * self.config.gradient_accumulation_steps\n",
    "    \n",
    "    def evaluate(self) -> float:\n",
    "        \"\"\"Evaluate model on validation set.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                x, y = batch\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                \n",
    "                outputs = self.model(x, labels=y)\n",
    "                loss = outputs['loss']\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Limit evaluation for faster training\n",
    "                if num_batches >= 50:\n",
    "                    break\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def save_checkpoint(self, filepath: str):\n",
    "        \"\"\"Save training checkpoint.\"\"\"\n",
    "        checkpoint = {\n",
    "            'step': self.step,\n",
    "            'epoch': self.epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'config': self.config\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "    \n",
    "    def generate_sample(self, prompt: str = \"The\", max_tokens: int = 128) -> str:\n",
    "        \"\"\"Generate a sample text during training.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        input_ids = torch.tensor([self.tokenizer.encode(prompt)]).to(self.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            generated = self.model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.8,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Main training loop.\"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Model parameters: {self.model.get_num_params():,}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        accumulated_loss = 0\n",
    "        \n",
    "        for epoch in range(self.config.max_epochs):\n",
    "            self.epoch = epoch\n",
    "            \n",
    "            for batch_idx, batch in enumerate(self.train_loader):\n",
    "                # Training step\n",
    "                loss = self.train_step(batch)\n",
    "                accumulated_loss += loss\n",
    "                \n",
    "                # Update parameters every gradient_accumulation_steps\n",
    "                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(),\n",
    "                        self.config.max_grad_norm\n",
    "                    )\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    self.step += 1\n",
    "                    \n",
    "                    # Track learning rate\n",
    "                    current_lr = self.scheduler.get_last_lr()[0]\n",
    "                    self.learning_rates.append(current_lr)\n",
    "                    \n",
    "                    # Logging\n",
    "                    if self.step % self.config.log_interval == 0:\n",
    "                        avg_loss = accumulated_loss / self.config.log_interval\n",
    "                        self.train_losses.append(avg_loss)\n",
    "                        \n",
    "                        elapsed = time.time() - start_time\n",
    "                        \n",
    "                        print(f\"Step {self.step:5d} | \"\n",
    "                              f\"Epoch {epoch:2d} | \"\n",
    "                              f\"Loss: {avg_loss:.4f} | \"\n",
    "                              f\"LR: {current_lr:.2e} | \"\n",
    "                              f\"Time: {elapsed:.1f}s\")\n",
    "                        \n",
    "                        accumulated_loss = 0\n",
    "                    \n",
    "                    # Evaluation\n",
    "                    if self.step % self.config.eval_interval == 0:\n",
    "                        val_loss = self.evaluate()\n",
    "                        self.val_losses.append(val_loss)\n",
    "                        \n",
    "                        print(f\"Validation loss: {val_loss:.4f}\")\n",
    "                        \n",
    "                        # Save best model\n",
    "                        if val_loss < self.best_val_loss:\n",
    "                            self.best_val_loss = val_loss\n",
    "                            self.save_checkpoint(f\"best_model_step_{self.step}.pt\")\n",
    "                            print(f\"New best model saved! Val loss: {val_loss:.4f}\")\n",
    "                        \n",
    "                        # Generate sample\n",
    "                        sample = self.generate_sample(\"The future of artificial intelligence\")\n",
    "                        print(f\"Sample: {sample[:100]}...\")\n",
    "                        print(\"-\" * 80)\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    if self.step % self.config.save_interval == 0:\n",
    "                        self.save_checkpoint(f\"checkpoint_step_{self.step}.pt\")\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    if self.config.max_steps and self.step >= self.config.max_steps:\n",
    "                        print(f\"Reached max steps ({self.config.max_steps})\")\n",
    "                        return\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        self.save_checkpoint(\"final_model.pt\")\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        \"\"\"Plot training and validation losses, plus learning rate schedule.\"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Training loss\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.train_losses)\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Steps (x100)')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Validation loss\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.val_losses)\n",
    "        plt.title('Validation Loss')\n",
    "        plt.xlabel('Evaluations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.learning_rates)\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Simplified GPT-2 model (use your full implementation)\n",
    "class GPT2Model(nn.Module):\n",
    "    \"\"\"Simplified GPT-2 for training demo.\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embeddings\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.h = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        # Final norm and head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Tie weights\n",
    "        self.lm_head.weight = self.wte.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        b, t = input_ids.size()\n",
    "        pos = torch.arange(0, t, device=input_ids.device).unsqueeze(0)\n",
    "        \n",
    "        tok_emb = self.wte(input_ids)\n",
    "        pos_emb = self.wpe(pos)\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return {'logits': logits, 'loss': loss}\n",
    "    \n",
    "    def generate(self, input_ids, max_new_tokens=50, temperature=1.0, do_sample=True):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                outputs = self.forward(input_ids)\n",
    "                logits = outputs['logits'][:, -1, :] / temperature\n",
    "                \n",
    "                if do_sample:\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                else:\n",
    "                    next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                \n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        return input_ids\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(config.max_position_embeddings, config.max_position_embeddings)).view(1, 1, config.max_position_embeddings, config.max_position_embeddings))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "        att = att.masked_fill(self.causal_mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        return self.c_proj(y)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Main Training Script\n",
    "# ================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    # Create training configuration\n",
    "    config = TrainingConfig(\n",
    "        # Model config (small for quick training)\n",
    "        n_embd=256,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        \n",
    "        # Training config\n",
    "        batch_size=8,  # Small batch for demo\n",
    "        learning_rate=1e-4,\n",
    "        max_epochs=3,\n",
    "        warmup_steps=500,\n",
    "        max_steps=20000,  # Limit for demo\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        # Data config\n",
    "        block_size=512,  # Smaller sequences for faster training\n",
    "        \n",
    "        # Logging\n",
    "        eval_interval=200,\n",
    "        log_interval=50,\n",
    "        save_interval=500\n",
    "    )\n",
    "    \n",
    "    # Create trainer and start training\n",
    "    trainer = TrainingManager(config)\n",
    "    trainer.train()\n",
    "    \n",
    "    # Plot results\n",
    "    trainer.plot_losses()\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal Evaluation:\")\n",
    "    final_val_loss = trainer.evaluate()\n",
    "    print(f\"Final validation loss: {final_val_loss:.4f}\")\n",
    "    \n",
    "    # Generate some samples\n",
    "    print(\"\\nGenerated Samples:\")\n",
    "    for prompt in [\"The\", \"In the future\", \"Artificial intelligence\"]:\n",
    "        sample = trainer.generate_sample(prompt, max_tokens=30)\n",
    "        print(f\"Prompt: '{prompt}' -> {sample}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1275933c",
   "metadata": {},
   "source": [
    "# GPT -2 V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708894e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "1. Increased block size to 512 to capture dependencies better\n",
    "2. Increased batch size, gradient accumulation\n",
    "3. Updated learning rate scheduler to use cosine decay after warmup (as suggested by DeepMind manager)\n",
    "4. Added Weights & Biases integration for better experiment tracking\n",
    "5. Implemented light/full checkpoint split for RunPod disk space management\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import wandb\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration parameters.\"\"\"\n",
    "    # Model config\n",
    "    vocab_size: int = 50257\n",
    "    max_position_embeddings: int = 512\n",
    "    n_embd: int = 256\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 8\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # Training config\n",
    "    batch_size: int = 8\n",
    "    learning_rate: float = 3e-4\n",
    "    max_epochs: int = 10\n",
    "    warmup_steps: int = 1000\n",
    "    max_steps: Optional[int] = None\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Data config\n",
    "    block_size: int = 512\n",
    "    \n",
    "    # Logging & Monitoring\n",
    "    eval_interval: int = 500\n",
    "    log_interval: int = 100\n",
    "    save_interval: int = 5000  # Full checkpoints every 5000 steps\n",
    "    light_save_interval: int = 1000  # Light checkpoints every 1000 steps\n",
    "    \n",
    "    # Weights & Biases\n",
    "    wandb_project: str = \"gpt2-training\"\n",
    "    wandb_name: Optional[str] = None\n",
    "    use_wandb: bool = True\n",
    "\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    \"\"\"Dataset for WikiText-103 with proper tokenization and chunking.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], tokenizer, block_size: int = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        # Tokenize all texts and concatenate\n",
    "        print(\"Tokenizing dataset...\")\n",
    "        all_tokens = []\n",
    "        \n",
    "        for text in texts:\n",
    "            if text.strip():  # Skip empty lines\n",
    "                tokens = tokenizer.encode(text.strip())\n",
    "                all_tokens.extend(tokens)\n",
    "                all_tokens.append(tokenizer.eos_token_id)  # Add EOS between articles\n",
    "        \n",
    "        self.tokens = torch.tensor(all_tokens, dtype=torch.long)\n",
    "        print(f\"Dataset has {len(self.tokens)} tokens\")\n",
    "        \n",
    "        # Calculate number of samples\n",
    "        self.num_samples = max(0, len(self.tokens) - block_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get a chunk of tokens\n",
    "        chunk = self.tokens[idx:idx + self.block_size + 1]\n",
    "        \n",
    "        # Input and target (shifted by 1)\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "\n",
    "class TrainingManager:\n",
    "    \"\"\"Manages the training process with W&B integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load and prepare data\n",
    "        self.train_loader, self.val_loader = self._prepare_data()\n",
    "        \n",
    "        # Initialize model\n",
    "        model_config = self._create_model_config()\n",
    "        self.model = GPT2Model(model_config).to(self.device)\n",
    "        print(f\"Model has {self.model.get_num_params():,} parameters\")\n",
    "        \n",
    "        # Initialize optimizer and scheduler\n",
    "        self.optimizer = self._create_optimizer()\n",
    "        self.scheduler = self._create_scheduler()\n",
    "        \n",
    "        # Training state\n",
    "        self.step = 0\n",
    "        self.epoch = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.learning_rates = []\n",
    "        \n",
    "        # Initialize Weights & Biases\n",
    "        if self.config.use_wandb:\n",
    "            self._init_wandb()\n",
    "    \n",
    "    def _init_wandb(self):\n",
    "        \"\"\"Initialize Weights & Biases logging.\"\"\"\n",
    "        # Create run name if not provided\n",
    "        if self.config.wandb_name is None:\n",
    "            self.config.wandb_name = f\"gpt2-{self.config.n_layer}L-{self.config.n_embd}d-{self.config.n_head}h\"\n",
    "        \n",
    "        # Initialize wandb\n",
    "        wandb.init(\n",
    "            project=self.config.wandb_project,\n",
    "            name=self.config.wandb_name,\n",
    "            config={\n",
    "                # Model config\n",
    "                \"vocab_size\": self.config.vocab_size,\n",
    "                \"max_position_embeddings\": self.config.max_position_embeddings,\n",
    "                \"n_embd\": self.config.n_embd,\n",
    "                \"n_layer\": self.config.n_layer,\n",
    "                \"n_head\": self.config.n_head,\n",
    "                \"dropout\": self.config.dropout,\n",
    "                \n",
    "                # Training config\n",
    "                \"batch_size\": self.config.batch_size,\n",
    "                \"learning_rate\": self.config.learning_rate,\n",
    "                \"max_epochs\": self.config.max_epochs,\n",
    "                \"warmup_steps\": self.config.warmup_steps,\n",
    "                \"max_steps\": self.config.max_steps,\n",
    "                \"gradient_accumulation_steps\": self.config.gradient_accumulation_steps,\n",
    "                \"weight_decay\": self.config.weight_decay,\n",
    "                \"max_grad_norm\": self.config.max_grad_norm,\n",
    "                \"block_size\": self.config.block_size,\n",
    "                \n",
    "                # Computed values\n",
    "                \"model_params\": self.model.get_num_params(),\n",
    "                \"device\": str(self.device)\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Watch model for gradients and parameters\n",
    "        wandb.watch(self.model, log=\"all\", log_freq=1000)\n",
    "        print(f\"Initialized W&B run: {wandb.run.name}\")\n",
    "    \n",
    "    def _create_model_config(self):\n",
    "        \"\"\"Create model configuration from training config.\"\"\"\n",
    "        from types import SimpleNamespace\n",
    "        return SimpleNamespace(\n",
    "            vocab_size=self.config.vocab_size,\n",
    "            max_position_embeddings=self.config.max_position_embeddings,\n",
    "            n_embd=self.config.n_embd,\n",
    "            n_layer=self.config.n_layer,\n",
    "            n_head=self.config.n_head,\n",
    "            n_inner=None,\n",
    "            dropout=self.config.dropout,\n",
    "            layer_norm_epsilon=1e-5,\n",
    "            use_bias=True\n",
    "        )\n",
    "    \n",
    "    def _prepare_data(self) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"Load and prepare WikiText-103 dataset.\"\"\"\n",
    "        print(\"Loading WikiText-103 dataset...\")\n",
    "        dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_texts = [item['text'] for item in dataset['train'] if item['text'].strip()]\n",
    "        val_texts = [item['text'] for item in dataset['validation'] if item['text'].strip()]\n",
    "        \n",
    "        train_dataset = WikiTextDataset(train_texts, self.tokenizer, self.config.block_size)\n",
    "        val_dataset = WikiTextDataset(val_texts, self.tokenizer, self.config.block_size)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "        print(f\"Val dataset: {len(val_dataset)} samples\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def _create_optimizer(self) -> optim.Optimizer:\n",
    "        \"\"\"Create AdamW optimizer with weight decay.\"\"\"\n",
    "        # Separate parameters for weight decay\n",
    "        decay_params = []\n",
    "        no_decay_params = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if any(nd in name for nd in ['bias', 'ln', 'norm']):\n",
    "                    no_decay_params.append(param)\n",
    "                else:\n",
    "                    decay_params.append(param)\n",
    "        \n",
    "        optimizer_groups = [\n",
    "            {'params': decay_params, 'weight_decay': self.config.weight_decay},\n",
    "            {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        \n",
    "        return optim.AdamW(optimizer_groups, lr=self.config.learning_rate)\n",
    "    \n",
    "    def _create_scheduler(self):\n",
    "        \"\"\"Create learning rate scheduler with warmup + cosine decay.\"\"\"\n",
    "        # Calculate total steps for cosine decay\n",
    "        if self.config.max_steps:\n",
    "            total_steps = self.config.max_steps\n",
    "        else:\n",
    "            total_steps = len(self.train_loader) * self.config.max_epochs // self.config.gradient_accumulation_steps\n",
    "        \n",
    "        print(f\"Total training steps: {total_steps}\")\n",
    "        print(f\"Warmup steps: {self.config.warmup_steps}\")\n",
    "        print(f\"Cosine decay steps: {total_steps - self.config.warmup_steps}\")\n",
    "        \n",
    "        def lr_lambda(step):\n",
    "            if step < self.config.warmup_steps:\n",
    "                # Linear warmup\n",
    "                return step / self.config.warmup_steps\n",
    "            else:\n",
    "                # Cosine decay from 1.0 to 0.0\n",
    "                progress = (step - self.config.warmup_steps) / (total_steps - self.config.warmup_steps)\n",
    "                progress = min(progress, 1.0)  # Clamp to avoid going beyond 1.0\n",
    "                return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
    "    \n",
    "    def save_light_checkpoint(self, filepath: str):\n",
    "        \"\"\"Save light checkpoint (just model weights + minimal state).\"\"\"\n",
    "        checkpoint = {\n",
    "            'step': self.step,\n",
    "            'epoch': self.epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Light checkpoint saved: {filepath}\")\n",
    "    \n",
    "    def save_full_checkpoint(self, filepath: str):\n",
    "        \"\"\"Save full checkpoint (everything needed for resuming training).\"\"\"\n",
    "        checkpoint = {\n",
    "            'step': self.step,\n",
    "            'epoch': self.epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'learning_rates': self.learning_rates,\n",
    "            'config': self.config\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Full checkpoint saved: {filepath}\")\n",
    "        \n",
    "        # Log checkpoint to W&B\n",
    "        if self.config.use_wandb:\n",
    "            wandb.save(filepath)\n",
    "    \n",
    "    def cleanup_old_checkpoints(self, keep_last_n=3):\n",
    "        \"\"\"Clean up old checkpoints to save disk space.\"\"\"\n",
    "        # Clean light checkpoints\n",
    "        light_files = glob.glob(\"light_checkpoint_step_*.pt\")\n",
    "        light_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        for old_file in light_files[:-keep_last_n]:\n",
    "            os.remove(old_file)\n",
    "            print(f\"Removed old checkpoint: {old_file}\")\n",
    "        \n",
    "        # Clean full checkpoints (keep fewer)\n",
    "        full_files = glob.glob(\"full_checkpoint_step_*.pt\")\n",
    "        full_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        for old_file in full_files[:-2]:  # Keep last 2 full checkpoints\n",
    "            os.remove(old_file)\n",
    "            print(f\"Removed old full checkpoint: {old_file}\")\n",
    "    \n",
    "    def train_step(self, batch) -> float:\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        self.model.train()\n",
    "        x, y = batch\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = self.model(x, labels=y)\n",
    "        loss = outputs['loss']\n",
    "        \n",
    "        # Scale loss for gradient accumulation\n",
    "        loss = loss / self.config.gradient_accumulation_steps\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss.item() * self.config.gradient_accumulation_steps\n",
    "    \n",
    "    def evaluate(self) -> float:\n",
    "        \"\"\"Evaluate model on validation set.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                x, y = batch\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                \n",
    "                outputs = self.model(x, labels=y)\n",
    "                loss = outputs['loss']\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Limit evaluation for faster training\n",
    "                if num_batches >= 50:\n",
    "                    break\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def generate_sample(self, prompt: str = \"The\", max_tokens: int = 50) -> str:\n",
    "        \"\"\"Generate a sample text during training.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        input_ids = torch.tensor([self.tokenizer.encode(prompt)]).to(self.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            generated = self.model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.8,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Main training loop with W&B logging.\"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Model parameters: {self.model.get_num_params():,}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        accumulated_loss = 0\n",
    "        \n",
    "        for epoch in range(self.config.max_epochs):\n",
    "            self.epoch = epoch\n",
    "            \n",
    "            for batch_idx, batch in enumerate(self.train_loader):\n",
    "                # Training step\n",
    "                loss = self.train_step(batch)\n",
    "                accumulated_loss += loss\n",
    "                \n",
    "                # Update parameters every gradient_accumulation_steps\n",
    "                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                    # Gradient clipping\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(),\n",
    "                        self.config.max_grad_norm\n",
    "                    )\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    self.step += 1\n",
    "                    \n",
    "                    # Track learning rate\n",
    "                    current_lr = self.scheduler.get_last_lr()[0]\n",
    "                    self.learning_rates.append(current_lr)\n",
    "                    \n",
    "                    # Logging\n",
    "                    if self.step % self.config.log_interval == 0:\n",
    "                        avg_loss = accumulated_loss / self.config.log_interval\n",
    "                        self.train_losses.append(avg_loss)\n",
    "                        \n",
    "                        elapsed = time.time() - start_time\n",
    "                        tokens_per_sec = (self.step * self.config.batch_size * self.config.block_size * self.config.gradient_accumulation_steps) / elapsed\n",
    "                        \n",
    "                        # Log to W&B\n",
    "                        if self.config.use_wandb:\n",
    "                            wandb.log({\n",
    "                                \"train/loss\": avg_loss,\n",
    "                                \"train/learning_rate\": current_lr,\n",
    "                                \"train/tokens_per_second\": tokens_per_sec,\n",
    "                                \"train/step\": self.step,\n",
    "                                \"train/epoch\": epoch\n",
    "                            })\n",
    "                        \n",
    "                        print(f\"Step {self.step:5d} | \"\n",
    "                              f\"Epoch {epoch:2d} | \"\n",
    "                              f\"Loss: {avg_loss:.4f} | \"\n",
    "                              f\"LR: {current_lr:.2e} | \"\n",
    "                              f\"Tokens/s: {tokens_per_sec:.0f} | \"\n",
    "                              f\"Time: {elapsed:.1f}s\")\n",
    "                        \n",
    "                        accumulated_loss = 0\n",
    "                    \n",
    "                    # Evaluation\n",
    "                    if self.step % self.config.eval_interval == 0:\n",
    "                        val_loss = self.evaluate()\n",
    "                        self.val_losses.append(val_loss)\n",
    "                        \n",
    "                        # Calculate perplexity for better interpretability\n",
    "                        train_ppl = math.exp(min(avg_loss if 'avg_loss' in locals() else 10, 10))\n",
    "                        val_ppl = math.exp(min(val_loss, 10))\n",
    "                        \n",
    "                        # Log to W&B\n",
    "                        if self.config.use_wandb:\n",
    "                            wandb.log({\n",
    "                                \"eval/loss\": val_loss,\n",
    "                                \"eval/perplexity\": val_ppl,\n",
    "                                \"train/perplexity\": train_ppl,\n",
    "                                \"eval/step\": self.step\n",
    "                            })\n",
    "                        \n",
    "                        print(f\"Validation loss: {val_loss:.4f} | Perplexity: {val_ppl:.2f}\")\n",
    "                        \n",
    "                        # Save best model\n",
    "                        if val_loss < self.best_val_loss:\n",
    "                            self.best_val_loss = val_loss\n",
    "                            self.save_full_checkpoint(f\"best_model_step_{self.step}.pt\")\n",
    "                            print(f\"New best model saved! Val loss: {val_loss:.4f}\")\n",
    "                            \n",
    "                            # Log best model to W&B\n",
    "                            if self.config.use_wandb:\n",
    "                                wandb.log({\"eval/best_loss\": val_loss})\n",
    "                        \n",
    "                        # Generate sample\n",
    "                        sample = self.generate_sample(\"The future of artificial intelligence\")\n",
    "                        print(f\"Sample: {sample[:100]}...\")\n",
    "                        \n",
    "                        # Log sample to W&B\n",
    "                        if self.config.use_wandb:\n",
    "                            wandb.log({\n",
    "                                \"samples/generation\": wandb.Html(f\"<p><b>Prompt:</b> 'The future of artificial intelligence'</p><p><b>Generated:</b> {sample}</p>\")\n",
    "                            })\n",
    "                        \n",
    "                        print(\"-\" * 80)\n",
    "                    \n",
    "                    # Save checkpoints with light/full split\n",
    "                    if self.step % self.config.light_save_interval == 0:\n",
    "                        self.save_light_checkpoint(f\"light_checkpoint_step_{self.step}.pt\")\n",
    "                        self.cleanup_old_checkpoints()\n",
    "                    \n",
    "                    if self.step % self.config.save_interval == 0:\n",
    "                        self.save_full_checkpoint(f\"full_checkpoint_step_{self.step}.pt\")\n",
    "                        self.cleanup_old_checkpoints()\n",
    "                    \n",
    "                    # Early stopping\n",
    "                    if self.config.max_steps and self.step >= self.config.max_steps:\n",
    "                        print(f\"Reached max steps ({self.config.max_steps})\")\n",
    "                        return\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        self.save_full_checkpoint(\"final_model.pt\")\n",
    "        self.cleanup_old_checkpoints()\n",
    "        \n",
    "        # Finish W&B run\n",
    "        if self.config.use_wandb:\n",
    "            wandb.finish()\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        \"\"\"Plot training and validation losses, plus learning rate schedule.\"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Training loss\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.train_losses)\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Steps (x100)')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Validation loss\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.val_losses)\n",
    "        plt.title('Validation Loss')\n",
    "        plt.xlabel('Evaluations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.learning_rates)\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class GPT2Model(nn.Module):\n",
    "    \"\"\"GPT-2 model implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embeddings\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.h = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        # Final norm and head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Tie weights\n",
    "        self.lm_head.weight = self.wte.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        b, t = input_ids.size()\n",
    "        pos = torch.arange(0, t, device=input_ids.device).unsqueeze(0)\n",
    "        \n",
    "        tok_emb = self.wte(input_ids)\n",
    "        pos_emb = self.wpe(pos)\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return {'logits': logits, 'loss': loss}\n",
    "    \n",
    "    def generate(self, input_ids, max_new_tokens=50, temperature=1.0, do_sample=True):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                outputs = self.forward(input_ids)\n",
    "                logits = outputs['logits'][:, -1, :] / temperature\n",
    "                \n",
    "                if do_sample:\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                else:\n",
    "                    next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                \n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        return input_ids\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(config.max_position_embeddings, config.max_position_embeddings)).view(1, 1, config.max_position_embeddings, config.max_position_embeddings))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "        att = att.masked_fill(self.causal_mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        return self.c_proj(y)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main training function.\"\"\"\n",
    "    # Create training configuration\n",
    "    config = TrainingConfig(\n",
    "        # Model config\n",
    "        n_embd=256,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        \n",
    "        # Training config\n",
    "        batch_size=8,\n",
    "        learning_rate=1e-4,\n",
    "        max_epochs=3,\n",
    "        warmup_steps=500,\n",
    "        max_steps=20000,\n",
    "        gradient_accumulation_steps=4,\n",
    "        \n",
    "        # Data config\n",
    "        block_size=512,\n",
    "        \n",
    "        # Logging\n",
    "        eval_interval=200,\n",
    "        log_interval=50,\n",
    "        save_interval=5000,\n",
    "        light_save_interval=1000,\n",
    "        \n",
    "        # W&B config\n",
    "        wandb_project=\"gpt2-deepmind\",\n",
    "        use_wandb=True\n",
    "    )\n",
    "    \n",
    "    # Create trainer and start training\n",
    "    trainer = TrainingManager(config)\n",
    "    trainer.train()\n",
    "    \n",
    "    # Plot results (optional, mainly for local debugging)\n",
    "    if not config.use_wandb:\n",
    "        trainer.plot_losses()\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal Evaluation:\")\n",
    "    final_val_loss = trainer.evaluate()\n",
    "    final_ppl = math.exp(min(final_val_loss, 10))\n",
    "    print(f\"Final validation loss: {final_val_loss:.4f}\")\n",
    "    print(f\"Final perplexity: {final_ppl:.2f}\")\n",
    "    \n",
    "    # Generate some samples\n",
    "    print(\"\\nGenerated Samples:\")\n",
    "    for prompt in [\"The\", \"In the future\", \"Artificial intelligence\"]:\n",
    "        sample = trainer.generate_sample(prompt, max_tokens=30)\n",
    "        print(f\"Prompt: '{prompt}' -> {sample}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f08a44e",
   "metadata": {},
   "source": [
    "# A little faster run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22bcbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "RTX 5090 OPTIMIZED FOR 5-HOUR TRAINING SESSION\n",
    "1. Massive batch sizes (64 x 8 = 512 effective batch)\n",
    "2. Mixed precision + torch.compile for max speed\n",
    "3. Target: ~6000 steps in 5 hours\n",
    "4. Same model architecture, just config changes\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import wandb\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Reduce memory/CPU overhead from HF tokenizers and avoid fork-related warnings\n",
    "os.environ.setdefault(\"TOKENIZERS_PARALLELISM\", \"false\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration parameters - RTX 5090 optimized for 5 hours.\"\"\"\n",
    "    # Model config - KEEP SAME AS ORIGINAL\n",
    "    vocab_size: int = 50257\n",
    "    max_position_embeddings: int = 512\n",
    "    n_embd: int = 256\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 8\n",
    "    dropout: float = 0.1\n",
    "    \n",
    "    # Training config - OPTIMIZED FOR RTX 5090 + 5 HOUR TARGET\n",
    "    batch_size: int = 64                 # 8x larger (was 8)\n",
    "    learning_rate: float = 3e-4\n",
    "    max_epochs: int = 10\n",
    "    warmup_steps: int = 500              # Faster warmup for 5h session\n",
    "    max_steps: Optional[int] = 6000      # Target for 5 hours\n",
    "    gradient_accumulation_steps: int = 8  # Effective batch = 64*8 = 512!\n",
    "    weight_decay: float = 0.01\n",
    "    max_grad_norm: float = 1.0\n",
    "    \n",
    "    # Data config\n",
    "    block_size: int = 512\n",
    "    stride: int = 512\n",
    "    \n",
    "    # Logging & Monitoring - More frequent for 5h session\n",
    "    eval_interval: int = 150             # Every 150 steps\n",
    "    log_interval: int = 20               # Every 20 steps  \n",
    "    save_interval: int = 1500            # Every 1500 steps\n",
    "    light_save_interval: int = 300       # Every 300 steps\n",
    "    \n",
    "    # RTX 5090 optimizations\n",
    "    use_mixed_precision: bool = True\n",
    "    compile_model: bool = True\n",
    "    \n",
    "    # Weights & Biases\n",
    "    wandb_project: str = \"gpt2-5hour-rtx5090\"\n",
    "    wandb_name: Optional[str] = None\n",
    "    use_wandb: bool = True\n",
    "\n",
    "\n",
    "class WikiTextDataset(Dataset):\n",
    "    \"\"\"Dataset with sliding-window chunking.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts: List[str], tokenizer, block_size: int = 512, stride: int = 512):\n",
    "        self.block_size = block_size\n",
    "        self.examples = []\n",
    "        print(\"Tokenizing dataset with sliding windows...\")\n",
    "        \n",
    "        # We create windows of length block_size + 1 so that x,y are both length block_size\n",
    "        window_len = block_size + 1\n",
    "        \n",
    "        for text in texts:\n",
    "            if not text.strip():\n",
    "                continue\n",
    "            \n",
    "            tokens = tokenizer.encode(text.strip())\n",
    "            seq_len = len(tokens)\n",
    "            if seq_len < window_len:\n",
    "                continue\n",
    "            \n",
    "            for i in range(0, seq_len - window_len + 1, stride):\n",
    "                chunk = tokens[i:i + window_len]\n",
    "                self.examples.append(torch.tensor(chunk, dtype=torch.long))\n",
    "            \n",
    "            # Add a tail window if leftover doesn't align with stride\n",
    "            if (seq_len - window_len) % stride != 0:\n",
    "                tail = tokens[-window_len:]\n",
    "                self.examples.append(torch.tensor(tail, dtype=torch.long))\n",
    "        \n",
    "        # Fallback: if per-text windows produced nothing, concatenate all tokens and window globally\n",
    "        if len(self.examples) == 0:\n",
    "            print(\"No chunks created per-text. Falling back to concatenated sliding windows across all texts.\")\n",
    "            all_tokens: List[int] = []\n",
    "            for text in texts:\n",
    "                t = text.strip()\n",
    "                if not t:\n",
    "                    continue\n",
    "                all_tokens.extend(tokenizer.encode(t))\n",
    "            if len(all_tokens) >= window_len:\n",
    "                for i in range(0, len(all_tokens) - window_len + 1, stride):\n",
    "                    chunk = all_tokens[i:i + window_len]\n",
    "                    self.examples.append(torch.tensor(chunk, dtype=torch.long))\n",
    "                if (len(all_tokens) - window_len) % stride != 0:\n",
    "                    tail = all_tokens[-window_len:]\n",
    "                    self.examples.append(torch.tensor(tail, dtype=torch.long))\n",
    "        \n",
    "        print(f\"Total chunks created: {len(self.examples)}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.examples[idx]\n",
    "        x = chunk[:-1]\n",
    "        y = chunk[1:]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "class TrainingManager:\n",
    "    \"\"\"Manages the training process with RTX 5090 optimizations.\"\"\"\n",
    "    \n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Initialize tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        # Silence long-sequence warnings; we chunk manually below\n",
    "        try:\n",
    "            self.tokenizer.model_max_length = int(1e9)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "        # Load and prepare data\n",
    "        self.train_loader, self.val_loader = self._prepare_data()\n",
    "        \n",
    "        # Initialize model\n",
    "        model_config = self._create_model_config()\n",
    "        self.model = GPT2Model(model_config).to(self.device)\n",
    "        \n",
    "        # RTX 5090 OPTIMIZATIONS\n",
    "        if config.use_mixed_precision:\n",
    "            self.scaler = torch.cuda.amp.GradScaler()\n",
    "            print(\"âœ… Mixed precision enabled for RTX 5090\")\n",
    "        else:\n",
    "            self.scaler = None\n",
    "            \n",
    "        if config.compile_model:\n",
    "            try:\n",
    "                self.model = torch.compile(self.model, mode=\"max-autotune\")\n",
    "                print(\"âœ… Model compiled with torch.compile\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Model compilation failed: {e}\")\n",
    "        \n",
    "        print(f\"ðŸš€ RTX 5090 Setup:\")\n",
    "        print(f\"   Model parameters: {self.model.get_num_params():,}\")\n",
    "        print(f\"   Batch size: {config.batch_size}\")\n",
    "        print(f\"   Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "        print(f\"   Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "        print(f\"   Target steps: {config.max_steps}\")\n",
    "        print(f\"   Estimated time: ~5 hours\")\n",
    "        \n",
    "        # Initialize optimizer and scheduler\n",
    "        self.optimizer = self._create_optimizer()\n",
    "        self.scheduler = self._create_scheduler()\n",
    "        \n",
    "        # Training state\n",
    "        self.step = 0\n",
    "        self.epoch = 0\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.learning_rates = []\n",
    "        \n",
    "        # Initialize Weights & Biases\n",
    "        if self.config.use_wandb:\n",
    "            self._init_wandb()\n",
    "    \n",
    "    def _init_wandb(self):\n",
    "        \"\"\"Initialize Weights & Biases logging.\"\"\"\n",
    "        # Create run name if not provided\n",
    "        if self.config.wandb_name is None:\n",
    "            self.config.wandb_name = f\"rtx5090-5h-{self.config.n_layer}L-{self.config.n_embd}d-{self.config.n_head}h\"\n",
    "        \n",
    "        # Initialize wandb\n",
    "        wandb.init(\n",
    "            project=self.config.wandb_project,\n",
    "            name=self.config.wandb_name,\n",
    "            config={\n",
    "                # Model config\n",
    "                \"vocab_size\": self.config.vocab_size,\n",
    "                \"max_position_embeddings\": self.config.block_size,\n",
    "                \"n_embd\": self.config.n_embd,\n",
    "                \"n_layer\": self.config.n_layer,\n",
    "                \"n_head\": self.config.n_head,\n",
    "                \"dropout\": self.config.dropout,\n",
    "                \n",
    "                # Training config\n",
    "                \"batch_size\": self.config.batch_size,\n",
    "                \"learning_rate\": self.config.learning_rate,\n",
    "                \"max_epochs\": self.config.max_epochs,\n",
    "                \"warmup_steps\": self.config.warmup_steps,\n",
    "                \"max_steps\": self.config.max_steps,\n",
    "                \"gradient_accumulation_steps\": self.config.gradient_accumulation_steps,\n",
    "                \"effective_batch_size\": self.config.batch_size * self.config.gradient_accumulation_steps,\n",
    "                \"weight_decay\": self.config.weight_decay,\n",
    "                \"max_grad_norm\": self.config.max_grad_norm,\n",
    "                \"block_size\": self.config.block_size,\n",
    "                \"stride\": self.config.stride,\n",
    "                \n",
    "                # RTX 5090 optimizations\n",
    "                \"use_mixed_precision\": self.config.use_mixed_precision,\n",
    "                \"compile_model\": self.config.compile_model,\n",
    "                \n",
    "                # Computed values\n",
    "                \"model_params\": self.model.get_num_params(),\n",
    "                \"device\": str(self.device),\n",
    "                \"target_training_time\": \"5 hours\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Watch model gradients\n",
    "        wandb.watch(self.model, log=\"gradients\", log_freq=500)\n",
    "        print(f\"âœ… W&B initialized: {wandb.run.name}\")\n",
    "    \n",
    "    def _create_model_config(self):\n",
    "        \"\"\"Create model configuration from training config.\"\"\"\n",
    "        from types import SimpleNamespace\n",
    "        return SimpleNamespace(\n",
    "            vocab_size=self.config.vocab_size,\n",
    "            max_position_embeddings=self.config.block_size,\n",
    "            n_embd=self.config.n_embd,\n",
    "            n_layer=self.config.n_layer,\n",
    "            n_head=self.config.n_head,\n",
    "            n_inner=None,\n",
    "            dropout=self.config.dropout,\n",
    "            layer_norm_epsilon=1e-5,\n",
    "            use_bias=True\n",
    "        )\n",
    "    \n",
    "    def _prepare_data(self) -> Tuple[DataLoader, DataLoader]:\n",
    "        \"\"\"Load and prepare WikiText-103 dataset.\"\"\"\n",
    "        print(\"Loading WikiText-103 dataset...\")\n",
    "        dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "        \n",
    "        # Create datasets\n",
    "        train_texts = [item['text'] for item in dataset['train'] if item['text'].strip()]\n",
    "        val_texts = [item['text'] for item in dataset['validation'] if item['text'].strip()]\n",
    "        \n",
    "        train_dataset = WikiTextDataset(\n",
    "            train_texts,\n",
    "            self.tokenizer,\n",
    "            block_size=self.config.block_size,\n",
    "            stride=self.config.stride,\n",
    "        )\n",
    "        val_dataset = WikiTextDataset(\n",
    "            val_texts,\n",
    "            self.tokenizer,\n",
    "            block_size=self.config.block_size,\n",
    "            stride=self.config.stride,\n",
    "        )\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "        print(f\"Val dataset: {len(val_dataset)} samples\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def _create_optimizer(self) -> optim.Optimizer:\n",
    "        \"\"\"Create AdamW optimizer with weight decay.\"\"\"\n",
    "        # Separate parameters for weight decay\n",
    "        decay_params = []\n",
    "        no_decay_params = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if any(nd in name for nd in ['bias', 'ln', 'norm']):\n",
    "                    no_decay_params.append(param)\n",
    "                else:\n",
    "                    decay_params.append(param)\n",
    "        \n",
    "        optimizer_groups = [\n",
    "            {'params': decay_params, 'weight_decay': self.config.weight_decay},\n",
    "            {'params': no_decay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        \n",
    "        return optim.AdamW(optimizer_groups, lr=self.config.learning_rate)\n",
    "    \n",
    "    def _create_scheduler(self):\n",
    "        \"\"\"Create learning rate scheduler with warmup + cosine decay.\"\"\"\n",
    "        # Calculate total steps for cosine decay\n",
    "        if self.config.max_steps:\n",
    "            total_steps = self.config.max_steps\n",
    "        else:\n",
    "            total_steps = len(self.train_loader) * self.config.max_epochs // self.config.gradient_accumulation_steps\n",
    "        \n",
    "        print(f\"Total training steps: {total_steps}\")\n",
    "        print(f\"Warmup steps: {self.config.warmup_steps}\")\n",
    "        print(f\"Cosine decay steps: {total_steps - self.config.warmup_steps}\")\n",
    "        \n",
    "        def lr_lambda(step):\n",
    "            if step < self.config.warmup_steps:\n",
    "                # Linear warmup\n",
    "                return step / self.config.warmup_steps\n",
    "            else:\n",
    "                # Cosine decay from 1.0 to 0.0\n",
    "                progress = (step - self.config.warmup_steps) / (total_steps - self.config.warmup_steps)\n",
    "                progress = min(progress, 1.0)  # Clamp to avoid going beyond 1.0\n",
    "                return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        return optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
    "    \n",
    "    def save_light_checkpoint(self, filepath: str):\n",
    "        \"\"\"Save light checkpoint (just model weights + minimal state).\"\"\"\n",
    "        checkpoint = {\n",
    "            'step': self.step,\n",
    "            'epoch': self.epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Light checkpoint saved: {filepath}\")\n",
    "    \n",
    "    def save_full_checkpoint(self, filepath: str):\n",
    "        \"\"\"Save full checkpoint (everything needed for resuming training).\"\"\"\n",
    "        checkpoint = {\n",
    "            'step': self.step,\n",
    "            'epoch': self.epoch,\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
    "            'best_val_loss': self.best_val_loss,\n",
    "            'train_losses': self.train_losses,\n",
    "            'val_losses': self.val_losses,\n",
    "            'learning_rates': self.learning_rates,\n",
    "            'config': self.config\n",
    "        }\n",
    "        torch.save(checkpoint, filepath)\n",
    "        print(f\"Full checkpoint saved: {filepath}\")\n",
    "        \n",
    "        # Log checkpoint to W&B\n",
    "        if self.config.use_wandb:\n",
    "            wandb.save(filepath)\n",
    "    \n",
    "    def cleanup_old_checkpoints(self, keep_last_n=3):\n",
    "        \"\"\"Clean up old checkpoints to save disk space.\"\"\"\n",
    "        # Clean light checkpoints\n",
    "        light_files = glob.glob(\"light_checkpoint_step_*.pt\")\n",
    "        light_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        for old_file in light_files[:-keep_last_n]:\n",
    "            os.remove(old_file)\n",
    "            print(f\"Removed old checkpoint: {old_file}\")\n",
    "        \n",
    "        # Clean full checkpoints (keep fewer)\n",
    "        full_files = glob.glob(\"full_checkpoint_step_*.pt\")\n",
    "        full_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
    "        for old_file in full_files[:-2]:  # Keep last 2 full checkpoints\n",
    "            os.remove(old_file)\n",
    "            print(f\"Removed old full checkpoint: {old_file}\")\n",
    "    \n",
    "    def train_step(self, batch) -> float:\n",
    "        \"\"\"RTX 5090 optimized training step with mixed precision.\"\"\"\n",
    "        self.model.train()\n",
    "        x, y = batch\n",
    "        x, y = x.to(self.device), y.to(self.device)\n",
    "        \n",
    "        # Use mixed precision if enabled\n",
    "        if self.scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = self.model(x, labels=y)\n",
    "                loss = outputs['loss'] / self.config.gradient_accumulation_steps\n",
    "            \n",
    "            # Scaled backward pass\n",
    "            self.scaler.scale(loss).backward()\n",
    "        else:\n",
    "            # Standard precision\n",
    "            outputs = self.model(x, labels=y)\n",
    "            loss = outputs['loss'] / self.config.gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def evaluate(self) -> float:\n",
    "        \"\"\"Evaluate model on validation set.\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.val_loader:\n",
    "                x, y = batch\n",
    "                x, y = x.to(self.device), y.to(self.device)\n",
    "                \n",
    "                # Use mixed precision for evaluation too\n",
    "                if self.scaler is not None:\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = self.model(x, labels=y)\n",
    "                        loss = outputs['loss']\n",
    "                else:\n",
    "                    outputs = self.model(x, labels=y)\n",
    "                    loss = outputs['loss']\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Limit evaluation for faster training\n",
    "                if num_batches >= 50:\n",
    "                    break\n",
    "        \n",
    "        if num_batches == 0:\n",
    "            print(\"Warning: Validation loader returned 0 batches. Returning inf loss.\")\n",
    "            return float('inf')\n",
    "        return total_loss / num_batches\n",
    "    \n",
    "    def generate_sample(self, prompt: str = \"The\", max_tokens: int = 50) -> str:\n",
    "        \"\"\"Generate a sample text during training.\"\"\"\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Tokenize prompt\n",
    "        input_ids = torch.tensor([self.tokenizer.encode(prompt)]).to(self.device)\n",
    "        \n",
    "        # Generate\n",
    "        with torch.no_grad():\n",
    "            generated = self.model.generate(\n",
    "                input_ids,\n",
    "                max_new_tokens=max_tokens,\n",
    "                temperature=0.8,\n",
    "                do_sample=True\n",
    "            )\n",
    "        \n",
    "        # Decode\n",
    "        generated_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"RTX 5090 optimized training loop for 5-hour session.\"\"\"\n",
    "        print(\"ðŸš€ Starting RTX 5090 - 5 Hour Training Session!\")\n",
    "        print(f\"Device: {self.device}\")\n",
    "        print(f\"Model parameters: {self.model.get_num_params():,}\")\n",
    "        print(f\"Effective batch size: {self.config.batch_size * self.config.gradient_accumulation_steps}\")\n",
    "        print(f\"Target: {self.config.max_steps} steps in ~5 hours\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        accumulated_loss = 0\n",
    "        \n",
    "        for epoch in range(self.config.max_epochs):\n",
    "            self.epoch = epoch\n",
    "            \n",
    "            for batch_idx, batch in enumerate(self.train_loader):\n",
    "                # Training step\n",
    "                loss = self.train_step(batch)\n",
    "                accumulated_loss += loss\n",
    "                \n",
    "                # Update parameters every gradient_accumulation_steps\n",
    "                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:\n",
    "                    # RTX 5090 optimized parameter update\n",
    "                    if self.scaler is not None:\n",
    "                        # Mixed precision optimizer step\n",
    "                        self.scaler.unscale_(self.optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "                    else:\n",
    "                        # Standard optimizer step\n",
    "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "                        self.optimizer.step()\n",
    "                    \n",
    "                    self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    self.step += 1\n",
    "                    \n",
    "                    # Track learning rate\n",
    "                    current_lr = self.scheduler.get_last_lr()[0]\n",
    "                    self.learning_rates.append(current_lr)\n",
    "                    \n",
    "                    # Logging\n",
    "                    if self.step % self.config.log_interval == 0:\n",
    "                        avg_loss = accumulated_loss / self.config.log_interval\n",
    "                        self.train_losses.append(avg_loss)\n",
    "                        \n",
    "                        elapsed = time.time() - start_time\n",
    "                        elapsed_hours = elapsed / 3600\n",
    "                        tokens_per_sec = (self.step * self.config.batch_size * self.config.block_size * self.config.gradient_accumulation_steps) / elapsed\n",
    "                        steps_per_hour = self.step / elapsed_hours if elapsed_hours > 0 else 0\n",
    "                        eta_hours = (self.config.max_steps - self.step) / steps_per_hour if steps_per_hour > 0 else 0\n",
    "                        \n",
    "                        # Log to W&B\n",
    "                        if self.config.use_wandb:\n",
    "                            wandb.log({\n",
    "                                \"train/loss\": avg_loss,\n",
    "                                \"train/learning_rate\": current_lr,\n",
    "                                \"train/tokens_per_second\": tokens_per_sec,\n",
    "                                \"train/steps_per_hour\": steps_per_hour,\n",
    "                                \"train/step\": self.step,\n",
    "                                \"train/epoch\": epoch,\n",
    "                                \"train/elapsed_hours\": elapsed_hours,\n",
    "                                \"train/eta_hours\": eta_hours,\n",
    "                                \"system/gpu_memory_gb\": torch.cuda.memory_allocated() / 1e9 if torch.cuda.is_available() else 0\n",
    "                            })\n",
    "                        \n",
    "                        print(f\"Step {self.step:5d} | \"\n",
    "                              f\"Loss: {avg_loss:.4f} | \"\n",
    "                              f\"LR: {current_lr:.2e} | \"\n",
    "                              f\"Tokens/s: {tokens_per_sec:.0f} | \"\n",
    "                              f\"Steps/h: {steps_per_hour:.0f} | \"\n",
    "                              f\"Time: {elapsed_hours:.1f}h | \"\n",
    "                              f\"ETA: {eta_hours:.1f}h | \"\n",
    "                              f\"GPU: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n",
    "                        \n",
    "                        accumulated_loss = 0\n",
    "                    \n",
    "                    # Evaluation\n",
    "                    if self.step % self.config.eval_interval == 0:\n",
    "                        val_loss = self.evaluate()\n",
    "                        self.val_losses.append(val_loss)\n",
    "                        \n",
    "                        # Calculate perplexity for better interpretability\n",
    "                        train_ppl = math.exp(min(avg_loss if 'avg_loss' in locals() else 10, 10))\n",
    "                        val_ppl = math.exp(min(val_loss, 10))\n",
    "                        \n",
    "                        # Log to W&B\n",
    "                        if self.config.use_wandb:\n",
    "                            wandb.log({\n",
    "                                \"eval/loss\": val_loss,\n",
    "                                \"eval/perplexity\": val_ppl,\n",
    "                                \"train/perplexity\": train_ppl,\n",
    "                                \"eval/step\": self.step\n",
    "                            })\n",
    "                        \n",
    "                        print(f\"ðŸŽ¯ Validation | Loss: {val_loss:.4f} | PPL: {val_ppl:.2f}\")\n",
    "                        \n",
    "                        # Save best model\n",
    "                        if val_loss < self.best_val_loss:\n",
    "                            self.best_val_loss = val_loss\n",
    "                            self.save_full_checkpoint(f\"best_model_step_{self.step}.pt\")\n",
    "                            print(f\"ðŸ† New best model! Val loss: {val_loss:.4f}\")\n",
    "                            \n",
    "                            # Log best model to W&B\n",
    "                            if self.config.use_wandb:\n",
    "                                wandb.log({\"eval/best_loss\": val_loss})\n",
    "                        \n",
    "                        # Generate sample\n",
    "                        sample = self.generate_sample(\"The future of AI\")\n",
    "                        print(f\"ðŸ“ Sample: {sample[:80]}...\")\n",
    "                        \n",
    "                        # Log sample to W&B\n",
    "                        if self.config.use_wandb:\n",
    "                            wandb.log({\n",
    "                                \"samples/generation\": wandb.Html(f\"<p><b>Prompt:</b> 'The future of AI'</p><p><b>Generated:</b> {sample}</p>\")\n",
    "                            })\n",
    "                        \n",
    "                        print(\"-\" * 80)\n",
    "                    \n",
    "                    # Save checkpoints\n",
    "                    if self.step % self.config.light_save_interval == 0:\n",
    "                        self.save_light_checkpoint(f\"light_checkpoint_step_{self.step}.pt\")\n",
    "                        self.cleanup_old_checkpoints()\n",
    "                    \n",
    "                    if self.step % self.config.save_interval == 0:\n",
    "                        self.save_full_checkpoint(f\"full_checkpoint_step_{self.step}.pt\")\n",
    "                        self.cleanup_old_checkpoints()\n",
    "                    \n",
    "                    # Check if 5 hours elapsed or max steps reached\n",
    "                    elapsed_hours = (time.time() - start_time) / 3600\n",
    "                    if elapsed_hours >= 5.0:\n",
    "                        print(f\"â° 5-hour time limit reached! ({elapsed_hours:.1f}h)\")\n",
    "                        break\n",
    "                        \n",
    "                    if self.config.max_steps and self.step >= self.config.max_steps:\n",
    "                        print(f\"âœ… Reached max steps ({self.config.max_steps})\")\n",
    "                        break\n",
    "            \n",
    "            # Check time limit again at epoch level\n",
    "            elapsed_hours = (time.time() - start_time) / 3600\n",
    "            if elapsed_hours >= 5.0 or (self.config.max_steps and self.step >= self.config.max_steps):\n",
    "                break\n",
    "        \n",
    "        # Training completed\n",
    "        total_time = time.time() - start_time\n",
    "        total_hours = total_time / 3600\n",
    "        total_tokens = self.step * self.config.batch_size * self.config.block_size * self.config.gradient_accumulation_steps\n",
    "        \n",
    "        print(\"ðŸŽ‰ RTX 5090 Training Session Complete!\")\n",
    "        print(f\"ðŸ“Š Final Stats:\")\n",
    "        print(f\"   Steps completed: {self.step}\")\n",
    "        print(f\"   Total time: {total_hours:.2f} hours\")\n",
    "        print(f\"   Average steps/hour: {self.step/total_hours:.0f}\")\n",
    "        print(f\"   Total tokens processed: {total_tokens:,}\")\n",
    "        print(f\"   Average tokens/sec: {total_tokens/total_time:.0f}\")\n",
    "        \n",
    "        self.save_full_checkpoint(\"final_5hour_model.pt\")\n",
    "        self.cleanup_old_checkpoints()\n",
    "        \n",
    "        # Finish W&B run\n",
    "        if self.config.use_wandb:\n",
    "            wandb.log({\n",
    "                \"final/total_steps\": self.step,\n",
    "                \"final/total_hours\": total_hours,\n",
    "                \"final/steps_per_hour\": self.step/total_hours,\n",
    "                \"final/total_tokens\": total_tokens,\n",
    "                \"final/tokens_per_second\": total_tokens/total_time\n",
    "            })\n",
    "            wandb.finish()\n",
    "    \n",
    "    def plot_losses(self):\n",
    "        \"\"\"Plot training and validation losses, plus learning rate schedule.\"\"\"\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Training loss\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.train_losses)\n",
    "        plt.title('Training Loss')\n",
    "        plt.xlabel('Steps (x20)')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Validation loss\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.val_losses)\n",
    "        plt.title('Validation Loss')\n",
    "        plt.xlabel('Evaluations')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Learning rate schedule\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.learning_rates)\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# KEEP ALL YOUR ORIGINAL MODEL CLASSES - NO CHANGES\n",
    "class GPT2Model(nn.Module):\n",
    "    \"\"\"GPT-2 model implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embeddings\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, config.n_embd)\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        \n",
    "        # Transformer blocks\n",
    "        self.h = nn.ModuleList([\n",
    "            TransformerBlock(config) for _ in range(config.n_layer)\n",
    "        ])\n",
    "        \n",
    "        # Final norm and head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Tie weights\n",
    "        self.lm_head.weight = self.wte.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        b, t = input_ids.size()\n",
    "        pos = torch.arange(0, t, device=input_ids.device).unsqueeze(0)\n",
    "        \n",
    "        tok_emb = self.wte(input_ids)\n",
    "        pos_emb = self.wpe(pos)\n",
    "        x = self.drop(tok_emb + pos_emb)\n",
    "        \n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "        \n",
    "        return {'logits': logits, 'loss': loss}\n",
    "    \n",
    "    def generate(self, input_ids, max_new_tokens=50, temperature=1.0, do_sample=True):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                outputs = self.forward(input_ids)\n",
    "                logits = outputs['logits'][:, -1, :] / temperature\n",
    "                \n",
    "                if do_sample:\n",
    "                    probs = torch.softmax(logits, dim=-1)\n",
    "                    next_token = torch.multinomial(probs, num_samples=1)\n",
    "                else:\n",
    "                    next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "                \n",
    "                input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        \n",
    "        return input_ids\n",
    "    \n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
    "        self.mlp = MLP(config)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        \n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.head_dim = config.n_embd // config.n_head\n",
    "        \n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        \n",
    "        self.register_buffer(\"causal_mask\", torch.tril(torch.ones(config.max_position_embeddings, config.max_position_embeddings)).view(1, 1, config.max_position_embeddings, config.max_position_embeddings))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        \n",
    "        qkv = self.c_attn(x)\n",
    "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
    "        \n",
    "        q = q.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(self.head_dim))\n",
    "        att = att.masked_fill(self.causal_mask[:, :, :T, :T] == 0, float('-inf'))\n",
    "        att = torch.softmax(att, dim=-1)\n",
    "        att = self.dropout(att)\n",
    "        \n",
    "        y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        return self.c_proj(y)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = torch.nn.functional.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"RTX 5090 - 5 Hour Training Session\"\"\"\n",
    "    # RTX 5090 optimized config for 5 hours\n",
    "    config = TrainingConfig(\n",
    "        # Keep your original model architecture\n",
    "        n_embd=256,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        \n",
    "        # RTX 5090 optimized for 5-hour session\n",
    "        batch_size=64,              # 8x bigger than L4\n",
    "        gradient_accumulation_steps=8, # Effective batch = 512\n",
    "        learning_rate=3e-4,\n",
    "        max_steps=6000,             # Target for 5 hours\n",
    "        warmup_steps=500,\n",
    "        \n",
    "        # Enable RTX 5090 optimizations  \n",
    "        use_mixed_precision=True,\n",
    "        compile_model=True,\n",
    "        \n",
    "        # 5-hour session monitoring\n",
    "        eval_interval=150,\n",
    "        log_interval=20,\n",
    "        save_interval=1500,\n",
    "        light_save_interval=300,\n",
    "        \n",
    "        wandb_project=\"gpt2-5hour-beast\",\n",
    "        use_wandb=True\n",
    "    )\n",
    "    \n",
    "    print(\"ðŸš€ RTX 5090 - 5 HOUR TRAINING SESSION\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"ðŸ“Š Configuration:\")\n",
    "    print(f\"   Model: {config.n_layer}L-{config.n_embd}d-{config.n_head}h\")\n",
    "    print(f\"   Batch size: {config.batch_size}\")\n",
    "    print(f\"   Gradient accumulation: {config.gradient_accumulation_steps}\")\n",
    "    print(f\"   Effective batch size: {config.batch_size * config.gradient_accumulation_steps}\")\n",
    "    print(f\"   Target steps: {config.max_steps}\")\n",
    "    print(f\"   Mixed precision: {config.use_mixed_precision}\")\n",
    "    print(f\"   Model compilation: {config.compile_model}\")\n",
    "    print(f\"   Time limit: 5 hours\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Estimate performance\n",
    "    print(f\"ðŸ”® Estimates (RTX 5090):\")\n",
    "    print(f\"   Expected speed: ~20 steps/min\")\n",
    "    print(f\"   Target in 5h: {config.max_steps} steps\")\n",
    "    print(f\"   Effective batch vs L4: 16x larger!\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create trainer and start\n",
    "    trainer = TrainingManager(config)\n",
    "    trainer.train()\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nðŸŽ¯ Final Results:\")\n",
    "    final_val_loss = trainer.evaluate()\n",
    "    final_ppl = math.exp(min(final_val_loss, 10))\n",
    "    print(f\"Final validation loss: {final_val_loss:.4f}\")\n",
    "    print(f\"Final perplexity: {final_ppl:.2f}\")\n",
    "    \n",
    "    # Generate samples\n",
    "    print(\"\\nðŸ“ Generated Samples:\")\n",
    "    for prompt in [\"The\", \"In the future\", \"Artificial intelligence\", \"RTX 5090\"]:\n",
    "        sample = trainer.generate_sample(prompt, max_tokens=30)\n",
    "        print(f\"'{prompt}' -> {sample}\")\n",
    "\n",
    "\n",
    "def continue_from_l4():\n",
    "    \"\"\"Continue training from L4 checkpoint on RTX 5090.\"\"\"\n",
    "    config = TrainingConfig(\n",
    "        # Same model architecture\n",
    "        n_embd=256,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        \n",
    "        # RTX 5090 optimized settings\n",
    "        batch_size=64,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-4,  # Slightly lower for continued training\n",
    "        max_steps=6000,      # Or continue to higher number\n",
    "        \n",
    "        use_mixed_precision=True,\n",
    "        compile_model=True,\n",
    "        \n",
    "        wandb_project=\"gpt2-5hour-continued\",\n",
    "        use_wandb=True\n",
    "    )\n",
    "    \n",
    "    trainer = TrainingManager(config)\n",
    "    \n",
    "    # Load L4 checkpoint\n",
    "    try:\n",
    "        # Update this path to your actual checkpoint\n",
    "        checkpoint_path = \"light_checkpoint_step_278.pt\"\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cuda')\n",
    "        trainer.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        trainer.step = checkpoint['step']\n",
    "        trainer.best_val_loss = checkpoint['best_val_loss']\n",
    "        print(f\"âœ… Loaded L4 checkpoint from step {trainer.step}\")\n",
    "        print(f\"ðŸš€ Continuing training on RTX 5090 with 16x larger batches!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"âš ï¸ No L4 checkpoint found, starting fresh\")\n",
    "    \n",
    "    trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose your approach:\n",
    "    \n",
    "    # Option 1: Fresh 5-hour training on RTX 5090 (recommended)\n",
    "    main()\n",
    "    \n",
    "    # Option 2: Continue from L4 checkpoint\n",
    "    # continue_from_l4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace22a80",
   "metadata": {},
   "source": [
    "# Before Moving Ahead we need to understand some more concepts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
