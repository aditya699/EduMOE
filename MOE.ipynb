{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348639bf",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff5bd6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Install Important Libs\n",
    "%pip install -q datasets transformers pydantic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a1a173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Literal\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709352ef",
   "metadata": {},
   "source": [
    "# Environment Check before running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6cf3fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library Versions: {'torch': '2.8.0.dev20250319+cu128', 'datasets': '4.0.0', 'transformers': '4.56.0'}\n",
      "Device Info: {'device': 'CPU', 'cuda_version': None, 'gpu_name': None, 'memory_gb': None, 'message': 'No GPU detected'}\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from typing import Optional, Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# --- Safe optional imports ---\n",
    "if importlib.util.find_spec(\"torch\"):\n",
    "    import torch\n",
    "else:\n",
    "    torch = None\n",
    "\n",
    "if importlib.util.find_spec(\"datasets\"):\n",
    "    import datasets\n",
    "else:\n",
    "    datasets = None\n",
    "\n",
    "if importlib.util.find_spec(\"transformers\"):\n",
    "    import transformers\n",
    "else:\n",
    "    transformers = None\n",
    "\n",
    "\n",
    "class LibraryVersions(BaseModel):\n",
    "    torch: str = \" Not Installed\"\n",
    "    datasets: str = \" Not Installed\"\n",
    "    transformers: str = \" Not Installed\"\n",
    "\n",
    "\n",
    "class GPUInfo(BaseModel):\n",
    "    device: Literal[\"GPU\", \"CPU\", \"No Device Detected\"]\n",
    "    cuda_version: Optional[str] = None\n",
    "    gpu_name: Optional[str] = None\n",
    "    memory_gb: Optional[float] = None\n",
    "    message: Optional[str] = None\n",
    "\n",
    "\n",
    "class EnvironmentCheck:\n",
    "    \"\"\"Environment checker with Pydantic models.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.torch_version = torch.__version__ if torch else None\n",
    "        self.datasets_version = datasets.__version__ if datasets else None\n",
    "        self.transformers_version = transformers.__version__ if transformers else None\n",
    "\n",
    "    def get_versions(self) -> LibraryVersions:\n",
    "        \"\"\"Return installed library versions as a Pydantic model.\"\"\"\n",
    "        return LibraryVersions(\n",
    "            torch=self.torch_version or \" Not Installed\",\n",
    "            datasets=self.datasets_version or \" Not Installed\",\n",
    "            transformers=self.transformers_version or \" Not Installed\",\n",
    "        )\n",
    "\n",
    "    def check_gpu(self) -> GPUInfo:\n",
    "        \"\"\"Return GPU details as a Pydantic model.\"\"\"\n",
    "        if not torch:\n",
    "            return GPUInfo(device=\"No Device Detected\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            props = torch.cuda.get_device_properties(0)\n",
    "            return GPUInfo(\n",
    "                device=\"GPU\",\n",
    "                cuda_version=torch.version.cuda,\n",
    "                gpu_name=torch.cuda.get_device_name(0),\n",
    "                memory_gb=round(props.total_memory / (1024**3), 2),\n",
    "            )\n",
    "        return GPUInfo(device=\"CPU\", message=\"No GPU detected\")\n",
    "\n",
    "\n",
    "# --- Usage for manual run ---\n",
    "if __name__ == \"__main__\":\n",
    "    env = EnvironmentCheck()\n",
    "    print(\"Library Versions:\", env.get_versions().model_dump())\n",
    "    print(\"Device Info:\", env.check_gpu().model_dump())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8814623",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b1c2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f95445c316146ef89aa3c3ced579dbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935d1dfeeb8d4a1682b6c80da2e0665d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ca40caa02340e287f143ab4c5f0b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412715ab8b3c448eb4ef0a2b29e7b90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bab08a0b6824053ae1987184e4aac6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9370654c10f462ba3b28e253f6d7ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35af765cf46f4be8a3187a9090313dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n",
      "Row 0: ''\n",
      "Row 1: ' = Valkyria Chronicles III = \\n'\n",
      "Row 2: ''\n",
      "Row 3: ' SenjÅ no Valkyria 3 : Unrecorded Chronicles ( Japanese : æˆ¦å ´ã®ãƒ´ã‚¡ãƒ«ã‚­ãƒ¥ãƒªã‚¢3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" Calamaty Raven \" . \\n'\n",
      "Row 4: \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . Character designer Raita Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"\n",
      "Row 5: \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\"\n",
      "Row 6: ''\n",
      "Row 7: ' = = Gameplay = = \\n'\n",
      "Row 8: ''\n",
      "Row 9: \" As with previous Valkyira Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main heroines , although they take a very minor role . \\n\"\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "\n",
    "class DatasetConfig(BaseModel):\n",
    "    \"\"\"Configuration model for dataset loading.\"\"\"\n",
    "    dataset_name: str = Field(..., description=\"Name of the dataset to load (e.g., 'wikitext').\")\n",
    "    subset_name: str = Field(..., description=\"Subset/config name (e.g., 'wikitext-2-raw-v1').\")\n",
    "\n",
    "\n",
    "class DatasetLoader:\n",
    "    \"\"\"Wrapper class for loading HuggingFace datasets.\"\"\"\n",
    "\n",
    "    def __init__(self, config: DatasetConfig) -> None:\n",
    "        self.config = config\n",
    "\n",
    "    def load_data(self) -> DatasetDict:\n",
    "        \"\"\"Load dataset using HuggingFace.\"\"\"\n",
    "        dataset = load_dataset(self.config.dataset_name, self.config.subset_name)\n",
    "        return dataset\n",
    "\n",
    "\n",
    "config = DatasetConfig(dataset_name=\"wikitext\", subset_name=\"wikitext-2-raw-v1\")\n",
    "loader = DatasetLoader(config)\n",
    "dataset = loader.load_data()\n",
    "\n",
    "print(dataset)\n",
    "for i in range(10):\n",
    "    print(f\"Row {i}: {dataset['train'][i]['text']!r}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59dea62",
   "metadata": {},
   "source": [
    "# Environment Setup and Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a430e52f",
   "metadata": {},
   "source": [
    "ðŸ”¹ Environment Setup\n",
    "\n",
    "Before training a language model, we must ensure the environment is ready:\n",
    "\n",
    "Libraries: PyTorch for neural networks, HuggingFace datasets for loading corpora, and transformers for tokenization.\n",
    "\n",
    "GPU Availability: Training transformers on CPU is not practical. We verified CUDA version, GPU type, and VRAM size.\n",
    "\n",
    "Reproducibility: Using a structured check (EnvironmentCheck), we can confirm dependencies and hardware setup are consistent across machines.\n",
    "\n",
    "ðŸ”¹ Dataset Preparation in LLM Training\n",
    "\n",
    "Large-scale models are trained on diverse, massive corpora:\n",
    "\n",
    "ðŸŒ Common Crawl â†’ large-scale web scrape (cleaned and deduplicated).\n",
    "\n",
    "ðŸ“š Books â†’ public-domain and licensed.\n",
    "\n",
    "ðŸ“ Wikipedia â†’ factual, well-structured text.\n",
    "\n",
    "ðŸ’» Code â†’ GitHub, forums, Q&A.\n",
    "\n",
    "ðŸ§ª Research Articles â†’ scientific sources like arXiv and PubMed.\n",
    "\n",
    "Such datasets often reach billions to trillions of tokens. Preprocessing includes deduplication, filtering, and quality checks.\n",
    "\n",
    "ðŸ”¹ Our Choice for EduMoE\n",
    "\n",
    "For this educational project, we use:\n",
    "\n",
    "âœ… WikiText-2 (raw):\n",
    "\n",
    "~2M tokens â€” small enough for fast experiments.\n",
    "\n",
    "Natural, factual prose similar to real-world corpora.\n",
    "\n",
    "Already split into train, validation, and test.\n",
    "\n",
    "Includes quirks like blank lines, which helps mimic real preprocessing challenges.\n",
    "\n",
    "This dataset is ideal for learning the mechanics of LLM training without overwhelming compute resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e21aa56",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cffb50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n",
      "Text: EduMoE is our Mixture of Experts project.\n",
      "Tokens: [36, 646, 16632, 36, 318, 674, 337, 9602, 286, 36095, 1628, 13]\n",
      "Decoded: EduMoE is our Mixture of Experts project.\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class TokenizerConfig(BaseModel):\n",
    "    \"\"\"Configuration for tokenizer setup.\"\"\"\n",
    "    tokenizer_name: str = Field(default=\"gpt2\", description=\"Pretrained tokenizer to use.\")\n",
    "    add_special_tokens: bool = Field(default=False, description=\"Whether to add special tokens like BOS/EOS.\")\n",
    "\n",
    "\n",
    "class TokenizerWrapper:\n",
    "    \"\"\"Wrapper for HuggingFace GPT-2 tokenizer.\"\"\"\n",
    "\n",
    "    def __init__(self, config: TokenizerConfig) -> None:\n",
    "        self.config = config\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_name)\n",
    "\n",
    "        # GPT-2 has no pad token by default â†’ we assign EOS as pad for batching\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"Convert text â†’ token IDs.\"\"\"\n",
    "        return self.tokenizer.encode(text, add_special_tokens=self.config.add_special_tokens)\n",
    "\n",
    "    def decode(self, tokens: List[int]) -> str:\n",
    "        \"\"\"Convert token IDs â†’ text.\"\"\"\n",
    "        return self.tokenizer.decode(tokens)\n",
    "\n",
    "    def vocab_size(self) -> int:\n",
    "        \"\"\"Return vocabulary size.\"\"\"\n",
    "        return self.tokenizer.vocab_size\n",
    "        \n",
    "\n",
    "# --- Usage ---\n",
    "if __name__ == \"__main__\":\n",
    "    config = TokenizerConfig(tokenizer_name=\"gpt2\", add_special_tokens=False)\n",
    "    tok = TokenizerWrapper(config)\n",
    "\n",
    "    text = \"EduMoE is our Mixture of Experts project.\"\n",
    "    tokens = tok.encode(text)\n",
    "    decoded = tok.decode(tokens)\n",
    "\n",
    "    print(\"Vocab size:\", tok.vocab_size())\n",
    "    print(\"Text:\", text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    print(\"Decoded:\", decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed66d6d1",
   "metadata": {},
   "source": [
    "# Tokenize WikiText-2 with Our Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c1d485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "  = Valkyria Chronicles III = \n",
      " \n",
      "\n",
      "Token IDs (first 40):\n",
      " [796, 569, 18354, 7496, 17740, 6711, 796, 220, 198] ...\n",
      "\n",
      "Decoded text:\n",
      "  = Valkyria Chronicles III = \n",
      " \n",
      "\n",
      "Char length vs Token length: 30 chars â†’ 9 tokens\n",
      "Tokenizer vocab size: 50257\n"
     ]
    }
   ],
   "source": [
    "# 1. Load WikiText-2 (raw)\n",
    "data_config = DatasetConfig(dataset_name=\"wikitext\", subset_name=\"wikitext-2-raw-v1\")\n",
    "dataset_loader = DatasetLoader(data_config)\n",
    "dataset = dataset_loader.load_data()\n",
    "\n",
    "# 2. Load GPT-2 Byte-Level BPE tokenizer\n",
    "tok_config = TokenizerConfig(tokenizer_name=\"gpt2\", add_special_tokens=False)\n",
    "tokenizer = TokenizerWrapper(tok_config)\n",
    "\n",
    "# 3. Grab a non-empty sample from training set\n",
    "sample_text = next(x[\"text\"] for x in dataset[\"train\"] if x[\"text\"].strip() != \"\")\n",
    "\n",
    "# 4. Encode â†’ token IDs\n",
    "token_ids = tokenizer.encode(sample_text)\n",
    "\n",
    "# 5. Decode â†’ back to text\n",
    "decoded_text = tokenizer.decode(token_ids)\n",
    "\n",
    "# 6. Print results\n",
    "print(\"Original text:\\n\", sample_text[:200], \"\\n\")\n",
    "print(\"Token IDs (first 40):\\n\", token_ids[:40], \"...\\n\")\n",
    "print(\"Decoded text:\\n\", decoded_text[:200], \"\\n\")\n",
    "print(\"Char length vs Token length:\", len(sample_text), \"chars â†’\", len(token_ids), \"tokens\")\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c007a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
