Tokenization = the process of mapping raw text (characters) into tokens (discrete units, integers).

Formally, we start with an alphabet (Î£, e.g., characters like aâ€“z) and map it into a vocabulary (Î”, tokens).

A tokenizer is a pair of mappings:

Encoder (Ï„): text â†’ tokens

Decoder (Îº): tokens â†’ text

In practice:

Input string "unbelievable"

Encoder â†’ [â€œunâ€, â€œbelievâ€, â€œableâ€]

IDs â†’ [1053, 8274, 2145]

Decoder maps back to "unbelievable".

ğŸ”¹ Why is Tokenization Needed?

Neural networks need numbers â†’ you canâ€™t feed raw text.

Vocabulary efficiency â†’ words are too many, characters are too small; subwords (BPE, Unigram, WordPiece) balance the tradeoff.

Open vocabulary â†’ tokenization allows us to handle unseen words by breaking them into subwords.

ğŸ”¹ Middle Ground: Subword Tokenization

Option 3: Subwords (BPE, WordPiece, Unigram)

"unbelievable" â†’ ["un", "believ", "able"]

Vocabulary size = ~30kâ€“50k (manageable).

Handles new words: â€œun+believ+ableâ€ instead of OOV.

Works across languages too.

This is why all modern LLMs use subword tokenization.

ğŸŸ¢ Step 2.2 â€” How Byte Pair Encoding (BPE) Works
ğŸ”¹ Intuition

BPE builds a subword vocabulary by iteratively merging frequent pairs of symbols.

Starts at the character level (so it can encode anything).

Gradually merges common pairs â†’ â€œsubwordsâ€ like th, ing, unbeliev.

Stops when vocab reaches a fixed size (e.g., 30k).